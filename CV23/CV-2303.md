- [ ] [\[2305.06144\] Learning Semi-supervised Gaussian Mixture Models for Generalized Category Discovery](https://arxiv.org/abs/2305.06144) (University of Edinburgh, ICCV)
- [ ] [\[2305.06200\] Learning in a Single Domain for Non-Stationary Multi-Texture Synthesis](https://arxiv.org/abs/2305.06200) (Illinois)
- [ ] [\[2305.06221\] Multi-Prompt with Depth Partitioned Cross-Modal Learning](https://arxiv.org/abs/2305.06221) (UCAS)
- [ ] [\[2305.06225\] DaGAN++: Depth-Aware Generative Adversarial Network for Talking Head Video Generation](https://arxiv.org/abs/2305.06225) (CVPR)
- [ ] [\[2305.06242\] Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving](https://arxiv.org/abs/2305.06242) (CVPR)
- [ ] [\[2305.06278\] A Multi-modal Garden Dataset and Hybrid 3D Dense Reconstruction Framework Based on Panoramic Stereo Images for a Trimming Robot](https://arxiv.org/abs/2305.06278) (USyd)
- [ ] [\[2305.06321\] SepMark: Deep Separable Watermarking for Unified Source Tracing and Deepfake Detection](https://arxiv.org/abs/2305.06321) (ACMMM)
- [ ] [\[2305.06356\] HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion](https://arxiv.org/abs/2305.06356) (TUM)
- [ ] [\[2305.06382\] HyperE2VID: Improving Event-Based Video Reconstruction via Hypernetworks](https://arxiv.org/abs/2305.06382) (TIP)
- [ ] [\[2305.06386\] Text-To-Concept (and Back) via Cross-Model Alignment](https://arxiv.org/abs/2305.06386) (UMD)
- [ ] [\[2305.06402\] Analyzing Bias in Diffusion-based Face Generation Models](https://arxiv.org/abs/2305.06402) (JHU)
- [ ] [\[2305.06448\] Continual Facial Expression Recognition: A Benchmark](https://arxiv.org/abs/2305.06448) (Cambridge)
- [ ] [\[2305.06456\] Perpetual Humanoid Control for Real-time Simulated Avatars](https://arxiv.org/abs/2305.06456) (ICCV)
- [ ] [\[2305.06500\] InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) (Salesforce)
- [ ] [\[2305.06525\] Pyramid Texture Filtering](https://arxiv.org/abs/2305.06525) (SIGGRAPH)
- [ ] [\[2305.06540\] Inter-frame Accelerate Attack against Video Interpolation Models](https://arxiv.org/abs/2305.06540) (UESTC)
- [ ] [\[2305.06559\] Patch-wise Mixed-Precision Quantization of Vision Transformer](https://arxiv.org/abs/2305.06559) (IA CAS)
- [ ] [\[2305.06564\] Undercover Deepfakes: Detecting Fake Segments in Videos](https://arxiv.org/abs/2305.06564) (NUS)
- [ ] [\[2305.06565\] Realization RGBD Image Stylization](https://arxiv.org/abs/2305.06565) (University of Alberta)
- [ ] [\[2305.06582\] Exploiting Fine-Grained DCT Representations for Hiding Image-Level Messages within JPEG Images](https://arxiv.org/abs/2305.06582) (ACMMM)
- [ ] [\[2305.06611\] Hyperbolic Deep Learning in Computer Vision: A Survey](https://arxiv.org/abs/2305.06611) (UVA.NL)
- [ ] [\[2305.06621\] PVT-SSD: Single-Stage 3D Object Detector with Point-Voxel Transformer](https://arxiv.org/abs/2305.06621) (CVPR)
- [ ] [\[2305.06671\] WeditGAN: Few-Shot Image Generation via Latent Space Relocation](https://arxiv.org/abs/2305.06671) (SJTU)
- [ ] [\[2305.06710\] Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator](https://arxiv.org/abs/2305.06710) (NUDT, ACMMM)
- [ ] [\[2305.06716\] Distracting Downpour: Adversarial Weather Attacks for Motion Estimation](https://arxiv.org/abs/2305.06716) (ICCV)
- [ ] [\[2305.06820\] DeepSTEP -- Deep Learning-Based Spatio-Temporal End-To-End Perception for Autonomous Vehicles](https://arxiv.org/abs/2305.06820) (TUM)
- [ ] [\[2305.06968\] HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation](https://arxiv.org/abs/2305.06968) (CVPR)
- [ ] [\[2305.06988\] Self-Chained Image-Language Model for Video Localization and Question Answering](https://arxiv.org/abs/2305.06988) (NIPS)
- [ ] [\[2305.07011\] Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2305.07011) (CVPR)
- [ ] [\[2305.07014\] Virtual Occlusions Through Implicit Depth](https://arxiv.org/abs/2305.07014) (CVPR)
- [ ] [\[2305.07015\] Exploiting Diffusion Prior for Real-World Image Super-Resolution](https://arxiv.org/abs/2305.07015) (NTU)
- [ ] [\[2305.07017\] An Inverse Scaling Law for CLIP Training](https://arxiv.org/abs/2305.07017) (NIPS)
- [ ] [\[2305.07027\] EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention](https://arxiv.org/abs/2305.07027) (CVPR)
- [ ] [\[2305.07214\] MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition](https://arxiv.org/abs/2305.07214) (CVPR)
- [ ] [\[2305.07239\] T-former: An Efficient Transformer for Image Inpainting](https://arxiv.org/abs/2305.07239) (XJTU)
- [ ] [\[2305.07304\] CLIP-Count: Towards Text-Guided Zero-Shot Object Counting](https://arxiv.org/abs/2305.07304) (PolyU, ACMMM)
- [ ] [\[2305.07307\] Self-Learning Symmetric Multi-view Probabilistic Clustering](https://arxiv.org/abs/2305.07307) (Alibaba)
- [ ] [\[2305.07328\] Configurable Spatial-Temporal Hierarchical Analysis for Flexible Video Anomaly Detection](https://arxiv.org/abs/2305.07328) (Fudan)
- [ ] [\[2305.07397\] Learning Monocular Depth in Dynamic Environment via Context-aware Temporal Attention](https://arxiv.org/abs/2305.07397) (Fudan)
- [ ] [\[2305.07514\] BlendFields: Few-Shot Example-Driven Facial Modeling](https://arxiv.org/abs/2305.07514) (CVPR)
- [ ] [\[2305.07528\] WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models](https://arxiv.org/abs/2305.07528) (CMU, CVPR)
- [ ] [\[2305.07540\] Content-based jewellery item retrieval using the local region-based histograms](https://arxiv.org/abs/2305.07540) (ZJU)
- [ ] [\[2305.07552\] Dish detection in food platters: A framework for automated diet logging and nutrition management](https://arxiv.org/abs/2305.07552) (ICCV)
- [ ] [\[2305.07602\] ViT Unified: Joint Fingerprint Recognition and Presentation Attack Detection](https://arxiv.org/abs/2305.07602) (Michigan State University)
- [ ] [\[2305.07613\] Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training](https://arxiv.org/abs/2305.07613) (CVPR)
- [ ] [\[2305.07618\] Uncertainty Estimation and Out-of-Distribution Detection for Deep Learning-Based Image Reconstruction using the Local Lipschitz](https://arxiv.org/abs/2305.07618) (BU)
- [ ] [\[2305.07625\] Meta Omnium: A Benchmark for General-Purpose Learning-to-Learn](https://arxiv.org/abs/2305.07625) (CVPR)
- [ ] [\[2305.07648\] A Critical View of Vision-Based Long-Term Dynamics Prediction Under Environment Misalignment](https://arxiv.org/abs/2305.07648) (ICML)
- [ ] [\[2305.07783\] ROI-based Deep Image Compression with Swin Transformers](https://arxiv.org/abs/2305.07783) (XJTU)
- [ ] [\[2305.07895\] OCRBench: On the Hidden Mystery of OCR in Large Multimodal Models](https://arxiv.org/abs/2305.07895) (HUST)
- [ ] [\[2305.07904\] Temporal Consistent Automatic Video Colorization via Semantic Correspondence](https://arxiv.org/abs/2305.07904) (BUPT)
- [ ] [\[2305.07920\] Multi-task Paired Masking with Alignment Modeling for Medical Vision-Language Pre-training](https://arxiv.org/abs/2305.07920) (ZJU)
- [ ] [\[2305.07943\] Illumination-insensitive Binary Descriptor for Visual Measurement Based on Local Inter-patch Invariance](https://arxiv.org/abs/2305.07943) (UESTC)
- [ ] [\[2305.08053\] SCRNet: a Retinex Structure-based Low-light Enhancement Model Guided by Spatial Consistency](https://arxiv.org/abs/2305.08053) (JHU)
- [ ] [\[2305.08066\] Helping Visually Impaired People Take Better Quality Pictures](https://arxiv.org/abs/2305.08066) (UT Austin)
- [ ] [\[2305.08069\] Instance-Aware Repeat Factor Sampling for Long-Tailed Object Detection](https://arxiv.org/abs/2305.08069) (Bosch)
- [ ] [\[2305.08075\] Analyzing Compression Techniques for Computer Vision](https://arxiv.org/abs/2305.08075) (UT Austin)
- [ ] [\[2305.08076\] Improving Defensive Distillation using Teacher Assistant](https://arxiv.org/abs/2305.08076) (UT Austin)
- [ ] [\[2305.08196\] A Comprehensive Survey on Segment Anything Model for Vision and Beyond](https://arxiv.org/abs/2305.08196) (SJTU)
- [ ] [\[2305.08252\] Parameter-Efficient Fine-Tuning for Medical Image Analysis: The Missed Opportunity](https://arxiv.org/abs/2305.08252) (University of Edinburgh)
- [ ] [\[2305.08275\] ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding](https://arxiv.org/abs/2305.08275) (Salesforce, CVPR)
- [ ] [\[2305.08293\] Identity-Preserving Talking Face Generation with Landmark and Appearance Priors](https://arxiv.org/abs/2305.08293) (CVPR)
- [ ] [\[2305.08302\] t-RAIN: Robust generalization under weather-aliasing label shift attacks](https://arxiv.org/abs/2305.08302) (CMU, CVPR)
- [ ] [\[2305.08318\] CMSG Cross-Media Semantic-Graph Feature Matching Algorithm for Autonomous Vehicle Relocalization](https://arxiv.org/abs/2305.08318) (USTC)
- [ ] [\[2305.08336\] Inverse Rendering of Translucent Objects using Physical and Neural Renderers](https://arxiv.org/abs/2305.08336) (CVPR)
- [ ] [\[2305.08381\] Parameter-efficient Tuning of Large-scale Multimodal Foundation Model](https://arxiv.org/abs/2305.08381) (NIPS)
- [ ] [\[2305.08389\] Edit As You Wish: Video Caption Editing with Multi-grained User Control](https://arxiv.org/abs/2305.08389) (Peking, ACMMM)
- [ ] [\[2305.08408\] SB-VQA: A Stack-Based Video Quality Assessment Framework for Video Enhancement](https://arxiv.org/abs/2305.08408) (CVPR)
- [ ] [\[2305.08413\] Artificial intelligence to advance Earth observation: a perspective](https://arxiv.org/abs/2305.08413) (EPFL)
- [ ] [\[2305.08455\] Document Understanding Dataset and Evaluation (DUDE)](https://arxiv.org/abs/2305.08455) (ICCV)
- [ ] [\[2305.08491\] Masked Collaborative Contrast for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2305.08491) (ZJU)
- [ ] [\[2305.08522\] Cross-Modality Time-Variant Relation Learning for Generating Dynamic Scene Graphs](https://arxiv.org/abs/2305.08522) (Tsinghua)
- [ ] [\[2305.08546\] Towards Visual Saliency Explanations of Face Verification](https://arxiv.org/abs/2305.08546) (EPFL)
- [ ] [\[2305.08590\] NIKI: Neural Inverse Kinematics with Invertible Neural Networks for 3D Human Pose and Shape Estimation](https://arxiv.org/abs/2305.08590) (CVPR)
- [ ] [\[2305.08611\] GeNAS: Neural Architecture Search with Better Generalization](https://arxiv.org/abs/2305.08611) (KAIST)
- [ ] [\[2305.08661\] Global and Local Mixture Consistency Cumulative Learning for Long-tailed Visual Recognitions](https://arxiv.org/abs/2305.08661) (CVPR)
- [ ] [\[2305.08675\] Improved baselines for vision-language pre-training](https://arxiv.org/abs/2305.08675) (Meta)
- [ ] [\[2305.08685\] CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding](https://arxiv.org/abs/2305.08685) (UCAS)
- [ ] [\[2305.08719\] M$^{6}$Doc: A Large-Scale Multi-Format, Multi-Type, Multi-Layout, Multi-Language, Multi-Annotation Category Dataset for Modern Document Layout Analysis](https://arxiv.org/abs/2305.08719) (CVPR)
- [ ] [\[2305.08721\] Learning More Discriminative Local Descriptors for Few-shot Learning](https://arxiv.org/abs/2305.08721) (UESTC)
- [ ] [\[2305.08776\] Bridging the Domain Gap: Self-Supervised 3D Scene Understanding with Foundation Models](https://arxiv.org/abs/2305.08776) (JHU, NIPS)
- [ ] [\[2305.08808\] GeoMAE: Masked Geometric Target Prediction for Self-supervised Point Cloud Pre-Training](https://arxiv.org/abs/2305.08808) (Tsinghua, CVPR)
- [ ] [\[2305.08810\] AutoRecon: Automated 3D Object Discovery and Reconstruction](https://arxiv.org/abs/2305.08810) (CVPR)
- [ ] [\[2305.08824\] Five A$^{+}$ Network: You Only Need 9K Parameters for Underwater Image Enhancement](https://arxiv.org/abs/2305.08824) (ZJU)
- [ ] [\[2305.08826\] Learning Better Contrastive View from Radiologist's Gaze](https://arxiv.org/abs/2305.08826) (NWPU)
- [ ] [\[2305.08851\] MV-Map: Offboard HD-Map Generation with Multi-view Consistency](https://arxiv.org/abs/2305.08851) (Fudan, ICCV)
- [ ] [\[2305.08938\] DopUS-Net: Quality-Aware Robotic Ultrasound Imaging based on Doppler Signal](https://arxiv.org/abs/2305.08938) (TUM)
- [ ] [\[2305.08953\] Motion Question Answering via Modular Motion Programs](https://arxiv.org/abs/2305.08953) (Stanford, ICML)
- [ ] [\[2305.08995\] Denoising Diffusion Models for Plug-and-Play Image Restoration](https://arxiv.org/abs/2305.08995) (ETH)
- [ ] [\[2305.09072\] Skin Deep: Investigating Subjectivity in Skin Tone Annotations for Computer Vision Benchmark Datasets](https://arxiv.org/abs/2305.09072) (UW)
- [ ] [\[2305.09073\] Consensus and Subjectivity of Skin Tone Annotation for ML Fairness](https://arxiv.org/abs/2305.09073) (Google)
- [ ] [\[2305.09078\] PanelNet: Understanding 360 Indoor Environment via Panel Representation](https://arxiv.org/abs/2305.09078) (CVPR)
- [ ] [\[2305.09095\] Multi-view MERA Subspace Clustering](https://arxiv.org/abs/2305.09095) (UESTC)
- [ ] [\[2305.09132\] DualGenerator: Information Interaction-based Generative Network for Point Cloud Completion](https://arxiv.org/abs/2305.09132) (XJTU)
- [ ] [\[2305.09160\] SUG: Single-dataset Unified Generalization for 3D Point Cloud Classification](https://arxiv.org/abs/2305.09160) (SJTU, ACMMM)
- [ ] [\[2305.09195\] Correlation Pyramid Network for 3D Single Object Tracking](https://arxiv.org/abs/2305.09195) (TUM)
- [ ] [\[2305.09236\] One-shot neural band selection for spectral recovery](https://arxiv.org/abs/2305.09236) (USTC)
- [ ] [\[2305.09293\] Out-of-Distribution Detection for Adaptive Computer Vision](https://arxiv.org/abs/2305.09293) (DLR)
- [ ] [\[2305.09305\] Releasing Inequality Phenomena in $L_{\infty}$-Adversarial Training via Input Gradient Distillation](https://arxiv.org/abs/2305.09305) (SYSU)
- [ ] [\[2305.09373\] Multi-task convolutional neural network for image aesthetic assessment](https://arxiv.org/abs/2305.09373) (KU Leuven)
- [ ] [\[2305.09447\] Multi-Level Global Context Cross Consistency Model for Semi-Supervised Ultrasound Image Segmentation with Diffusion Model](https://arxiv.org/abs/2305.09447) (HIT)
- [ ] [\[2305.09454\] Rethinking the editing of generative adversarial networks: a method to estimate editing vectors based on dimension reduction](https://arxiv.org/abs/2305.09454) (ShanghaiTech)
- [ ] [\[2305.09512\] Light-VQA: A Multi-Dimensional Quality Assessment Model for Low-Light Video Enhancement](https://arxiv.org/abs/2305.09512) (SJTU)
- [ ] [\[2305.09527\] Learning Correspondence Uncertainty via Differentiable Nonlinear Least Squares](https://arxiv.org/abs/2305.09527) (MIT)
- [ ] [\[2305.09533\] NightHazeFormer: Single Nighttime Haze Removal Using Prior Query Transformer](https://arxiv.org/abs/2305.09533) (HKUST(GZ))
- [ ] [\[2305.09641\] FitMe: Deep Photorealistic 3D Morphable Model Avatars](https://arxiv.org/abs/2305.09641) (CVPR)
- [ ] [\[2305.09664\] Understanding 3D Object Interaction from a Single Image](https://arxiv.org/abs/2305.09664) (University of Michigan, ICCV)
- [ ] [\[2305.09699\] Mobile User Interface Element Detection Via Adaptively Prompt Tuning](https://arxiv.org/abs/2305.09699) (CVPR)
- [ ] [\[2305.09746\] A Range-Null Space Decomposition Approach for Fast and Flexible Spectral Compressive Imaging](https://arxiv.org/abs/2305.09746) (HUST)
- [ ] [\[2305.09828\] Mimetic Initialization of Self-Attention Layers](https://arxiv.org/abs/2305.09828) (CMU)
- [ ] [\[2305.09924\] CageViT: Convolutional Activation Guided Efficient Vision Transformer](https://arxiv.org/abs/2305.09924) (NIPS)
- [ ] [\[2305.09972\] Real-Time Flying Object Detection with YOLOv8](https://arxiv.org/abs/2305.09972) (GIT)
- [ ] [\[2305.10018\] Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers](https://arxiv.org/abs/2305.10018) (AWS)
- [ ] [\[2305.10029\] TextSLAM: Visual SLAM with Semantic Planar Text Features](https://arxiv.org/abs/2305.10029) (SJTU)
- [ ] [\[2305.10049\] TG-VQA: Ternary Game of Video Question Answering](https://arxiv.org/abs/2305.10049) (Peking)
- [ ] [\[2305.10061\] Rethinking Boundary Discontinuity Problem for Oriented Object Detection](https://arxiv.org/abs/2305.10061) (CVPR)
- [ ] [\[2305.10082\] Imbalanced Aircraft Data Anomaly Detection](https://arxiv.org/abs/2305.10082) (NWPU)
- [ ] [\[2305.10146\] CS-PCN: Context-Space Progressive Collaborative Network for Image Denoising](https://arxiv.org/abs/2305.10146) (Nankai)
- [ ] [\[2305.10260\] From Region to Patch: Attribute-Aware Foreground-Background Contrastive Learning for Fine-Grained Fashion Retrieval](https://arxiv.org/abs/2305.10260) (ZJU)
- [ ] [\[2305.10289\] Explain Any Concept: Segment Anything Meets Concept-Based Explanation](https://arxiv.org/abs/2305.10289) (Illinois)
- [ ] [\[2305.10293\] Infinite Class Mixup](https://arxiv.org/abs/2305.10293) (Google)
- [ ] [\[2305.10299\] Binarized Spectral Compressive Imaging](https://arxiv.org/abs/2305.10299) (NIPS)
- [ ] [\[2305.10311\] Investigating image-based fallow weed detection performance on Raphanus sativus and Avena sativa at speeds up to 30 km h$^{-1}$](https://arxiv.org/abs/2305.10311) (USyd)
- [ ] [\[2305.10319\] Automatic Photo Orientation Detection with Convolutional Neural Networks](https://arxiv.org/abs/2305.10319) (University of Toronto)
- [ ] [\[2305.10320\] CostFormer:Cost Transformer for Cost Aggregation in Multi-view Stereo](https://arxiv.org/abs/2305.10320) (Alibaba)
- [ ] [\[2305.10420\] CLIP-GCD: Simple Language Guided Generalized Category Discovery](https://arxiv.org/abs/2305.10420) (GIT)
- [ ] [\[2305.10424\] ZeroFlow: Scalable Scene Flow via Distillation](https://arxiv.org/abs/2305.10424) (ICLR)
- [ ] [\[2305.10431\] FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention](https://arxiv.org/abs/2305.10431) (MIT)
- [ ] [\[2305.10462\] DualVector: Unsupervised Vector Font Synthesis with Dual-Part Representation](https://arxiv.org/abs/2305.10462) (CVPR)
- [ ] [\[2305.10465\] Towards Robust Probabilistic Modeling on SO(3) via Rotation Laplace Distribution](https://arxiv.org/abs/2305.10465) (TPAMI)
- [ ] [\[2305.10469\] Object Segmentation by Mining Cross-Modal Semantics](https://arxiv.org/abs/2305.10469) (HIT, ACMMM)
- [ ] [\[2305.10474\] Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models](https://arxiv.org/abs/2305.10474) (ICCV)
- [ ] [\[2305.10503\] OR-NeRF: Object Removing from 3D Scenes Guided by Multiview Segmentation with Neural Radiance Fields](https://arxiv.org/abs/2305.10503) (NTU)
- [ ] [\[2305.10507\] ReasonNet: End-to-End Driving with Temporal and Global Reasoning](https://arxiv.org/abs/2305.10507) (CVPR)
- [ ] [\[2305.10552\] Deep Multiple Instance Learning with Distance-Aware Self-Attention](https://arxiv.org/abs/2305.10552) (Cambridge)
- [ ] [\[2305.10593\] Inverted Non-maximum Suppression for more Accurate and Neater Face Detection](https://arxiv.org/abs/2305.10593) (Tongji)
- [ ] [\[2305.10648\] Adjusting Logit in Gaussian Form for Long-Tailed Visual Recognition](https://arxiv.org/abs/2305.10648) (Xiamen, CVPR)
- [ ] [\[2305.10657\] PTQD: Accurate Post-Training Quantization for Diffusion Models](https://arxiv.org/abs/2305.10657) (NIPS)
- [ ] [\[2305.10662\] Private Gradient Estimation is Useful for Generative Modeling](https://arxiv.org/abs/2305.10662) (USTC, ACMMM)
- [ ] [\[2305.10675\] Tuned Contrastive Learning](https://arxiv.org/abs/2305.10675) (UCSD)
- [ ] [\[2305.10683\] Paxion: Patching Action Knowledge in Video-Language Foundation Models](https://arxiv.org/abs/2305.10683) (NIPS)
- [ ] [\[2305.10714\] Vision-Language Pre-training with Object Contrastive Learning for 3D Scene Understanding](https://arxiv.org/abs/2305.10714) (HIT)
- [ ] [\[2305.10727\] Boost Vision Transformer with GPU-Friendly Sparsity and Quantization](https://arxiv.org/abs/2305.10727) (Fudan, CVPR)
- [ ] [\[2305.10794\] Multi-spectral Class Center Network for Face Manipulation Detection and Localization](https://arxiv.org/abs/2305.10794) (USTC)
- [ ] [\[2305.10799\] MedBLIP: Bootstrapping Language-Image Pre-training from 3D Medical Images and Texts](https://arxiv.org/abs/2305.10799) (SJTU)
- [ ] [\[2305.10825\] DiffUTE: Universal Text Editing Diffusion Model](https://arxiv.org/abs/2305.10825) (NIPS)
- [ ] [\[2305.10843\] X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models](https://arxiv.org/abs/2305.10843) (HKUST(GZ), NIPS)
- [ ] [\[2305.10854\] 3D Registration with Maximal Cliques](https://arxiv.org/abs/2305.10854) (CVPR)
- [ ] [\[2305.10855\] TextDiffuser: Diffusion Models as Text Painters](https://arxiv.org/abs/2305.10855) (HKUST, NIPS)
- [ ] [\[2305.10856\] Spatial-Frequency Discriminability for Revealing Adversarial Perturbations](https://arxiv.org/abs/2305.10856) (SYSU)
- [ ] [\[2305.10874\] Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation](https://arxiv.org/abs/2305.10874) (Microsoft)
- [ ] [\[2305.10899\] Ultra-High Resolution Segmentation with Ultra-Rich Context: A Novel Benchmark](https://arxiv.org/abs/2305.10899) (USTC, CVPR)
- [ ] [\[2305.10925\] Unsupervised Hyperspectral Pansharpening via Low-rank Diffusion Model](https://arxiv.org/abs/2305.10925) (XJTU)
- [ ] [\[2305.10973\] Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold](https://arxiv.org/abs/2305.10973) (SIGGRAPH)
- [ ] [\[2305.10983\] Assessor360: Multi-sequence Network for Blind Omnidirectional Image Quality Assessment](https://arxiv.org/abs/2305.10983) (University of Tokyo)
- [ ] [\[2305.11012\] SDC-UDA: Volumetric Unsupervised Domain Adaptation Framework for Slice-Direction Continuous Cross-Modality Medical Image Segmentation](https://arxiv.org/abs/2305.11012) (CVPR)
- [ ] [\[2305.11067\] Generating coherent comic with rich story using ChatGPT and Stable Diffusion](https://arxiv.org/abs/2305.11067) (University of Toronto)
- [ ] [\[2305.11147\] UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild](https://arxiv.org/abs/2305.11147) (Stanford, NIPS)
- [ ] [\[2305.11167\] MVPSNet: Fast Generalizable Multi-view Photometric Stereo](https://arxiv.org/abs/2305.11167) (UMD)
- [ ] [\[2305.11175\] VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks](https://arxiv.org/abs/2305.11175) (Tsinghua)
- [ ] [\[2305.11281\] SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models](https://arxiv.org/abs/2305.11281) (NIPS)
- [ ] [\[2305.11321\] JoIN: Joint GANs Inversion for Intrinsic Image Decomposition](https://arxiv.org/abs/2305.11321) (Illinois)
- [ ] [\[2305.11421\] PastNet: Introducing Physical Inductive Biases for Spatio-temporal Video Prediction](https://arxiv.org/abs/2305.11421) (USTC)
- [ ] [\[2305.11443\] Equivariant Multi-Modality Image Fusion](https://arxiv.org/abs/2305.11443) (XJTU, CVPR)
- [ ] [\[2305.11468\] Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness](https://arxiv.org/abs/2305.11468) (CMU)
- [ ] [\[2305.11481\] CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image Segmentation](https://arxiv.org/abs/2305.11481) (IA CAS)
- [ ] [\[2305.11490\] LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation](https://arxiv.org/abs/2305.11490) (ICLR)
- [ ] [\[2305.11497\] TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding](https://arxiv.org/abs/2305.11497) (HKUST)
- [ ] [\[2305.11513\] When SAM Meets Shadow Detection](https://arxiv.org/abs/2305.11513) (BU)
- [ ] [\[2305.11520\] LaCon: Late-Constraint Diffusion for Steerable Guided Image Synthesis](https://arxiv.org/abs/2305.11520) (USTC)
- [ ] [\[2305.11522\] DSFNet: Dual Space Fusion Network for Occlusion-Robust 3D Dense Face Alignment](https://arxiv.org/abs/2305.11522) (NUS, CVPR)
- [ ] [\[2305.11540\] Efficient Cross-Lingual Transfer for Chinese Stable Diffusion with Images as Pivots](https://arxiv.org/abs/2305.11540) (Tsinghua)
- [ ] [\[2305.11577\] LeftRefill: Filling Right Canvas based on Left Reference through Generalized Text-to-Image Diffusion Model](https://arxiv.org/abs/2305.11577) (CVPR)
- [ ] [\[2305.11588\] Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields](https://arxiv.org/abs/2305.11588) (HKUST)
- [ ] [\[2305.11601\] Towards Better Gradient Consistency for Neural Signed Distance Functions via Level Set Alignment](https://arxiv.org/abs/2305.11601) (Tsinghua, CVPR)
- [ ] [\[2305.11664\] Few-shot 3D Shape Generation](https://arxiv.org/abs/2305.11664) (Tsinghua)
- [ ] [\[2305.11675\] Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity](https://arxiv.org/abs/2305.11675) (CUHK)
- [ ] [\[2305.11676\] Learning Global-aware Kernel for Image Harmonization](https://arxiv.org/abs/2305.11676) (ZJU)
- [ ] [\[2305.11692\] Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery](https://arxiv.org/abs/2305.11692) (UCL)
- [ ] [\[2305.11716\] Efficient and Deterministic Search Strategy Based on Residual Projections for Point Cloud Registration with Correspondences](https://arxiv.org/abs/2305.11716) (TUM)
- [ ] [\[2305.11718\] Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization](https://arxiv.org/abs/2305.11718) (CVPR)
- [ ] [\[2305.11719\] Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling](https://arxiv.org/abs/2305.11719) (NUS)
- [ ] [\[2305.11733\] Long-tailed Visual Recognition via Gaussian Clouded Logit Adjustment](https://arxiv.org/abs/2305.11733) (Xiamen, CVPR)
- [ ] [\[2305.11768\] Generating Visual Spatial Description via Holistic 3D Scene Understanding](https://arxiv.org/abs/2305.11768) (HIT)
- [ ] [\[2305.11769\] Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner](https://arxiv.org/abs/2305.11769) (IA CAS, ACMMM)
- [ ] [\[2305.11818\] MaGIC: Multi-modality Guided Image Completion](https://arxiv.org/abs/2305.11818) (UCAS)
- [ ] [\[2305.11846\] Any-to-Any Generation via Composable Diffusion](https://arxiv.org/abs/2305.11846) (Microsoft)
- [ ] [\[2305.11875\] FR-Net:A Light-weight FFT Residual Net For Gaze Estimation](https://arxiv.org/abs/2305.11875) (NWPU)
- [ ] [\[2305.11884\] Novel deep learning methods for 3D flow field segmentation and classification](https://arxiv.org/abs/2305.11884) (UESTC)
- [ ] [\[2305.12015\] Inventing art styles with no artistic training data](https://arxiv.org/abs/2305.12015) (Berkeley)
- [ ] [\[2305.12032\] The Waymo Open Sim Agents Challenge](https://arxiv.org/abs/2305.12032) (NIPS)
- [ ] [\[2305.12106\] Human-annotated label noise and their impact on ConvNets for remote sensing image scene classification](https://arxiv.org/abs/2305.12106) (PolyU)
- [ ] [\[2305.12144\] DiffCap: Exploring Continuous Diffusion on Image Captioning](https://arxiv.org/abs/2305.12144) (Peking)
- [ ] [\[2305.12218\] Text-Video Retrieval with Disentangled Conceptualization and Set-to-Set Alignment](https://arxiv.org/abs/2305.12218) (Tsinghua)
- [ ] [\[2305.12328\] InstructVid2Vid: Controllable Video Editing with Natural Language Instructions](https://arxiv.org/abs/2305.12328) (NUS)
- [ ] [\[2305.12361\] A Dual-level Detection Method for Video Copy Detection](https://arxiv.org/abs/2305.12361) (USTC)
- [ ] [\[2305.12369\] HIINT: Historical, Intra- and Inter- personal Dynamics Modeling with Cross-person Memory Transformer](https://arxiv.org/abs/2305.12369) (MIT)
- [ ] [\[2305.12398\] Language Knowledge-Assisted Representation Learning for Skeleton-Based Action Recognition](https://arxiv.org/abs/2305.12398) (Xidian)
- [ ] [\[2305.12427\] VL-Fields: Towards Language-Grounded Neural Implicit Spatial Representations](https://arxiv.org/abs/2305.12427) (University of Edinburgh)
- [ ] [\[2305.12437\] SCP: Soft Conditional Prompt Learning for Aerial Video Action Recognition](https://arxiv.org/abs/2305.12437) (UMD)
- [ ] [\[2305.12452\] Advancing Referring Expression Segmentation Beyond Single Image](https://arxiv.org/abs/2305.12452) (ZJU)
- [ ] [\[2305.12476\] Zero-shot Visual Relation Detection via Composite Visual Cues from Large Language Models](https://arxiv.org/abs/2305.12476) (HKUST)
- [ ] [\[2305.12497\] PanoContext-Former: Panoramic Total Scene Understanding with a Transformer](https://arxiv.org/abs/2305.12497) (Alibaba)
- [ ] [\[2305.12529\] DreamWaltz: Make a Scene with Complex 3D Animatable Avatars](https://arxiv.org/abs/2305.12529) (NIPS)
- [ ] [\[2305.12554\] CoMusion: Towards Consistent Stochastic Human Motion Prediction via Motion Diffusion](https://arxiv.org/abs/2305.12554) (ECCV)
- [ ] [\[2305.12577\] Guided Motion Diffusion for Controllable Human Motion Synthesis](https://arxiv.org/abs/2305.12577) (ICCV)
- [ ] [\[2305.12596\] iWarpGAN: Disentangling Identity and Style to Generate Synthetic Iris Images](https://arxiv.org/abs/2305.12596) (Michigan State University)
- [ ] [\[2305.12683\] Mist: Towards Improved Adversarial Examples for Diffusion Models](https://arxiv.org/abs/2305.12683) (SJTU)
- [ ] [\[2305.12691\] Hi-ResNet: Edge Detail Enhancement for High-Resolution Remote Sensing Segmentation](https://arxiv.org/abs/2305.12691) (USyd)
- [ ] [\[2305.12711\] Unsupervised Visible-Infrared Person ReID by Collaborative Learning with Neighbor-Guided Label Refinement](https://arxiv.org/abs/2305.12711) (Xidian)
- [ ] [\[2305.12716\] The CLIP Model is Secretly an Image-to-Prompt Converter](https://arxiv.org/abs/2305.12716) (Xidian, NIPS)
- [ ] [\[2305.12726\] Towards Explainable In-the-Wild Video Quality Assessment: A Database and a Language-Prompted Approach](https://arxiv.org/abs/2305.12726) (NTU, ACMMM)
- [ ] [\[2305.12743\] Semantic Invariant Multi-view Clustering with Fully Incomplete Information](https://arxiv.org/abs/2305.12743) (Tianjin)
- [ ] [\[2305.12833\] Boosting Long-tailed Object Detection via Step-wise Learning on Smooth-tail Data](https://arxiv.org/abs/2305.12833) (NUS)
- [ ] [\[2305.12843\] Registering Neural Radiance Fields as 3D Density Images](https://arxiv.org/abs/2305.12843) (HKUST)
- [ ] [\[2305.12852\] Cycle Consistency-based Uncertainty Quantification of Neural Networks in Inverse Imaging Problems](https://arxiv.org/abs/2305.12852) (UCLA)
- [ ] [\[2305.12876\] Gloss-Free End-to-End Sign Language Translation](https://arxiv.org/abs/2305.12876) (ZJU)
- [ ] [\[2305.12943\] Album Storytelling with Iterative Story-aware Captioning and Large Language Models](https://arxiv.org/abs/2305.12943) (Peking)
- [ ] [\[2305.12954\] Is Synthetic Data From Diffusion Models Ready for Knowledge Distillation?](https://arxiv.org/abs/2305.12954) (UCL)
- [ ] [\[2305.12961\] Enhanced Meta Label Correction for Coping with Label Corruption](https://arxiv.org/abs/2305.12961) (ICCV)
- [ ] [\[2305.12964\] Text-based Person Search without Parallel Image-Text Data](https://arxiv.org/abs/2305.12964) (IA CAS, ACMMM)
- [ ] [\[2305.12966\] Hierarchical Integration Diffusion Model for Realistic Image Deblurring](https://arxiv.org/abs/2305.12966) (SJTU, NIPS)
- [ ] [\[2305.12972\] VanillaNet: the Power of Minimalism in Deep Learning](https://arxiv.org/abs/2305.12972) (USyd)
- [ ] [\[2305.13031\] HGFormer: Hierarchical Grouping Transformer for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2305.13031) (CVPR)
- [ ] [\[2305.13055\] Parallelizing Optical Flow Estimation on an Ultra-Low Power RISC-V Cluster for Nano-UAV Navigation](https://arxiv.org/abs/2305.13055) (ETH)
- [ ] [\[2305.13173\] Semantic-Promoted Debiasing and Background Disambiguation for Zero-Shot Instance Segmentation](https://arxiv.org/abs/2305.13173) (CVPR)
- [ ] [\[2305.13220\] Fast Monocular Scene Reconstruction with Global-Sparse Local-Dense Grids](https://arxiv.org/abs/2305.13220) (CVPR)
- [ ] [\[2305.13232\] Revisiting Data Augmentation in Model Compression: An Empirical and Comprehensive Study](https://arxiv.org/abs/2305.13232) (Tsinghua)
- [ ] [\[2305.13277\] U-TILISE: A Sequence-to-sequence Model for Cloud Removal in Optical Satellite Time Series](https://arxiv.org/abs/2305.13277) (ETH)
- [ ] [\[2305.13284\] Target-Aware Generative Augmentations for Single-Shot Adaptation](https://arxiv.org/abs/2305.13284) (ICML)
- [ ] [\[2305.13291\] Materialistic: Selecting Similar Materials in Images](https://arxiv.org/abs/2305.13291) (MIT)
- [ ] [\[2305.13310\] Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching](https://arxiv.org/abs/2305.13310) (ICLR)
- [ ] [\[2305.13311\] VDT: General-purpose Video Diffusion Transformers via Mask Modeling](https://arxiv.org/abs/2305.13311) (Berkeley)
- [ ] [\[2305.13312\] Contextualising Implicit Representations for Semantic Tasks](https://arxiv.org/abs/2305.13312) (Oxford)
- [ ] [\[2305.13353\] RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars](https://arxiv.org/abs/2305.13353) (CUHK)
- [ ] [\[2305.13398\] nnDetection for Intracranial Aneurysms Detection and Localization](https://arxiv.org/abs/2305.13398) (UW)
- [ ] [\[2305.13456\] Revisiting pre-trained remote sensing model benchmarks: resizing and normalization matters](https://arxiv.org/abs/2305.13456) (Microsoft)
- [ ] [\[2305.13495\] Type-to-Track: Retrieve Any Object via Prompt-based Tracking](https://arxiv.org/abs/2305.13495) (CMU, NIPS)
- [ ] [\[2305.13500\] Learning Emotion Representations from Verbal and Nonverbal Communication](https://arxiv.org/abs/2305.13500) (CVPR)
- [ ] [\[2305.13501\] LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On](https://arxiv.org/abs/2305.13501) (ACMMM)
- [ ] [\[2305.13520\] Tied-Augment: Controlling Representation Similarity Improves Data Augmentation](https://arxiv.org/abs/2305.13520) (Stanford, ICML)
- [ ] [\[2305.13548\] Attribute-Guided Encryption with Facial Texture Masking](https://arxiv.org/abs/2305.13548) (JHU)
- [ ] [\[2305.13570\] Cross-source Point Cloud Registration: Challenges, Progress and Prospects](https://arxiv.org/abs/2305.13570) (UTS)
- [ ] [\[2305.13600\] SiCL: Silhouette-Driven Contrastive Learning for Unsupervised Person Re-Identification with Clothes Change](https://arxiv.org/abs/2305.13600) (Tsinghua)
- [ ] [\[2305.13605\] Adaptive Face Recognition Using Adversarial Information Network](https://arxiv.org/abs/2305.13605) (TIP)
- [ ] [\[2305.13607\] Not All Image Regions Matter: Masked Vector Quantization for Autoregressive Image Generation](https://arxiv.org/abs/2305.13607) (USTC, CVPR)
- [ ] [\[2305.13611\] A New Comprehensive Benchmark for Semi-supervised Video Anomaly Detection and Anticipation](https://arxiv.org/abs/2305.13611) (NWPU, CVPR)
- [ ] [\[2305.13622\] Continual Learning with Strong Experience Replay](https://arxiv.org/abs/2305.13622) (NUS)
- [ ] [\[2305.13655\] LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models](https://arxiv.org/abs/2305.13655) (Berkeley)
- [ ] [\[2305.13665\] Dual Focal Loss for Calibration](https://arxiv.org/abs/2305.13665) (USyd, ICML)
- [ ] [\[2305.13770\] MIPI 2023 Challenge on Nighttime Flare Removal: Methods and Results](https://arxiv.org/abs/2305.13770) (NTU)
- [ ] [\[2305.13778\] Full Resolution Repetition Counting](https://arxiv.org/abs/2305.13778) (SJTU)
- [ ] [\[2305.13786\] Perception Test: A Diagnostic Benchmark for Multimodal Video Models](https://arxiv.org/abs/2305.13786) (Google, NIPS)
- [ ] [\[2305.13803\] NORM: Knowledge Distillation via N-to-One Representation Matching](https://arxiv.org/abs/2305.13803) (ICLR)
- [ ] [\[2305.13814\] Leveraging BEV Representation for 360-degree Visual Place Recognition](https://arxiv.org/abs/2305.13814) (ZJU)
- [ ] [\[2305.13839\] SAR-to-Optical Image Translation via Thermodynamics-inspired Network](https://arxiv.org/abs/2305.13839) (Xidian)
- [ ] [\[2305.13864\] MIANet: Aggregating Unbiased Instance and General Information for Few-Shot Semantic Segmentation](https://arxiv.org/abs/2305.13864) (CVPR)
- [ ] [\[2305.13872\] Variational Bayesian Framework for Advanced Image Generation with Domain-Related Variables](https://arxiv.org/abs/2305.13872) (Tsinghua)
- [ ] [\[2305.13880\] Generalized Expectation Maximization Framework for Blind Image Super Resolution](https://arxiv.org/abs/2305.13880) (Tsinghua)
- [ ] [\[2305.14053\] Parts of Speech-Grounded Subspaces in Vision-Language Models](https://arxiv.org/abs/2305.14053) (QMUL, NIPS)
- [ ] [\[2305.14059\] Accelerated Coordinate Encoding: Learning to Relocalize in Minutes using RGB and Poses](https://arxiv.org/abs/2305.14059) (CVPR)
- [ ] [\[2305.14093\] Weakly Supervised 3D Open-vocabulary Segmentation](https://arxiv.org/abs/2305.14093) (NIPS)
- [ ] [\[2305.14095\] S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions](https://arxiv.org/abs/2305.14095) (University of Michigan, NIPS)
- [ ] [\[2305.14100\] ISP: Multi-Layered Garment Draping with Implicit Sewing Patterns](https://arxiv.org/abs/2305.14100) (EPFL, NIPS)
- [ ] [\[2305.14165\] Impact of Light and Shadow on Robustness of Deep Neural Networks](https://arxiv.org/abs/2305.14165) (ZJU)
- [ ] [\[2305.14167\] DetGPT: Detect What You Need via Reasoning](https://arxiv.org/abs/2305.14167) (HKU)
- [ ] [\[2305.14236\] REC-MV: REconstructing 3D Dynamic Cloth from Monocular Videos](https://arxiv.org/abs/2305.14236) (CVPR)
- [ ] [\[2305.14268\] Masked Path Modeling for Vision-and-Language Navigation](https://arxiv.org/abs/2305.14268) (AWS)
- [ ] [\[2305.14306\] Hierarchical Adaptive Voxel-guided Sampling for Real-time Applications in Large-scale Point Clouds](https://arxiv.org/abs/2305.14306) (HIT)
- [ ] [\[2305.14334\] Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence](https://arxiv.org/abs/2305.14334) (NIPS)
- [ ] [\[2305.14335\] Prototype Adaption and Projection for Few- and Zero-shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2305.14335) (NTU, TIP)
- [ ] [\[2305.14344\] Siamese Masked Autoencoders](https://arxiv.org/abs/2305.14344) (Stanford)
- [ ] [\[2305.14352\] An Extensible Multimodal Multi-task Object Dataset with Materials](https://arxiv.org/abs/2305.14352) (Stanford, ICLR)
- [ ] [\[2305.14395\] Towards credible visual model interpretation with path attribution](https://arxiv.org/abs/2305.14395) (ICML)
- [ ] [\[2305.14428\] Prompting Language-Informed Distribution for Compositional Zero-Shot Learning](https://arxiv.org/abs/2305.14428) (ECCV)
- [ ] [\[2305.14486\] Point2SSM: Learning Morphological Variations of Anatomies from Point Cloud](https://arxiv.org/abs/2305.14486) (ICLR)
- [ ] [\[2305.14522\] Design a Delicious Lunchbox in Style](https://arxiv.org/abs/2305.14522) (CVPR)
- [ ] [\[2305.14551\] Exploring Semantic Variations in GAN Latent Spaces via Matrix Factorization](https://arxiv.org/abs/2305.14551) (ICLR)
- [ ] [\[2305.14575\] Towards Early Prediction of Human iPSC Reprogramming Success](https://arxiv.org/abs/2305.14575) (University of Alberta)
- [ ] [\[2305.14598\] Vision + Language Applications: A Survey](https://arxiv.org/abs/2305.14598) (CVPR)
- [ ] [\[2305.14637\] Learning UI-to-Code Reverse Generator Using Visual Critic Without Rendering](https://arxiv.org/abs/2305.14637) (UMD)
- [ ] [\[2305.14668\] NOVUM: Neural Object Volumes for Robust Object Classification](https://arxiv.org/abs/2305.14668) (ECCV)
- [ ] [\[2305.14674\] T1: Scaling Diffusion Probabilistic Fields to High-Resolution on Unified Visual Modalities](https://arxiv.org/abs/2305.14674) (JHU)
- [ ] [\[2305.14677\] Optimal Linear Subspace Search: Learning to Construct Fast and High-Quality Schedulers for Diffusion Models](https://arxiv.org/abs/2305.14677) (Alibaba)
- [ ] [\[2305.14691\] Label-Efficient Learning in Agriculture: A Comprehensive Review](https://arxiv.org/abs/2305.14691) (Michigan State University)
- [ ] [\[2305.14708\] EgoVSR: Towards High-Quality Egocentric Video Super-Resolution](https://arxiv.org/abs/2305.14708) (Tsinghua)
- [ ] [\[2305.14715\] Leveraging Future Relationship Reasoning for Vehicle Trajectory Prediction](https://arxiv.org/abs/2305.14715) (ICLR)
- [ ] [\[2305.14742\] ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation](https://arxiv.org/abs/2305.14742) (Peking)
- [ ] [\[2305.14758\] MRN: Multiplexed Routing Network for Incremental Multilingual Text Recognition](https://arxiv.org/abs/2305.14758) (ICCV)
- [ ] [\[2305.14779\] Alt-Text with Context: Improving Accessibility for Images on Twitter](https://arxiv.org/abs/2305.14779) (CMU, ICLR)
- [ ] [\[2305.14800\] Exploring Diverse In-Context Configurations for Image Captioning](https://arxiv.org/abs/2305.14800) (NIPS)
- [ ] [\[2305.14813\] Semi-Supervised and Long-Tailed Object Detection with CascadeMatch](https://arxiv.org/abs/2305.14813) (NTU)
- [ ] [\[2305.14831\] OD-NeRF: Efficient Training of On-the-Fly Dynamic Neural Radiance Fields](https://arxiv.org/abs/2305.14831) (NUS)
- [ ] [\[2305.14836\] NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario](https://arxiv.org/abs/2305.14836) (Fudan)
- [ ] [\[2305.14840\] Predicting Token Impact Towards Efficient Vision Transformer](https://arxiv.org/abs/2305.14840) (Fudan)
- [ ] [\[2305.14846\] Introducing Competition to Boost the Transferability of Targeted Adversarial Examples through Clean Feature Mixup](https://arxiv.org/abs/2305.14846) (CVPR)
- [ ] [\[2305.14885\] Towards View-invariant and Accurate Loop Detection Based on Scene Graph](https://arxiv.org/abs/2305.14885) (HKUST)
- [ ] [\[2305.14890\] HARD: Hard Augmentations for Robust Distillation](https://arxiv.org/abs/2305.14890) (University of Göttingen)
- [ ] [\[2305.14914\] GAMUS: A Geometry-aware Multi-modal Semantic Segmentation Benchmark for Remote Sensing Data](https://arxiv.org/abs/2305.14914) (TUM)
- [ ] [\[2305.14918\] Incremental Dense Reconstruction from Monocular Video with Guided Sparse Feature Volume Fusion](https://arxiv.org/abs/2305.14918) (TUM)
- [ ] [\[2305.14955\] DC-Net: Divide-and-Conquer for Salient Object Detection](https://arxiv.org/abs/2305.14955) (MBZUAI)
- [ ] [\[2305.14969\] MMNet: Multi-Mask Network for Referring Image Segmentation](https://arxiv.org/abs/2305.14969) (IA CAS)
- [ ] [\[2305.14985\] IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models](https://arxiv.org/abs/2305.14985) (HKUST)
- [ ] [\[2305.15030\] Make Lossy Compression Meaningful for Low-Light Images](https://arxiv.org/abs/2305.15030) (Peking)
- [ ] [\[2305.15078\] UniINR: Event-guided Unified Rolling Shutter Correction, Deblurring, and Interpolation](https://arxiv.org/abs/2305.15078) (University of Tokyo, ECCV)
- [ ] [\[2305.15086\] Unpaired Image-to-Image Translation via Neural Schr\"odinger Bridge](https://arxiv.org/abs/2305.15086) (ICLR)
- [ ] [\[2305.15094\] InNeRF360: Text-Guided 3D-Consistent Object Inpainting on 360-degree Neural Radiance Fields](https://arxiv.org/abs/2305.15094) (CVPR)
- [ ] [\[2305.15114\] Thinking Twice: Clinical-Inspired Thyroid Ultrasound Lesion Detection Based on Feature Feedback](https://arxiv.org/abs/2305.15114) (HIT)
- [ ] [\[2305.15134\] Networks are Slacking Off: Understanding Generalization Problem in Image Deraining](https://arxiv.org/abs/2305.15134) (USyd, NIPS)
- [ ] [\[2305.15171\] Deceptive-NeRF/3DGS: Diffusion-Generated Pseudo-Observations for High-Quality Sparse-View Reconstruction](https://arxiv.org/abs/2305.15171) (UCSD, ECCV)
- [ ] [\[2305.15213\] GTNet: Graph Transformer Network for 3D Point Cloud Classification and Semantic Segmentation](https://arxiv.org/abs/2305.15213) (NTU)
- [ ] [\[2305.15241\] Robust Classification via a Single Diffusion Model](https://arxiv.org/abs/2305.15241) (Tsinghua, ICML)
- [ ] [\[2305.15270\] Reversible Graph Neural Network-based Reaction Distribution Learning for Multiple Appropriate Facial Reactions Generation](https://arxiv.org/abs/2305.15270) (Cambridge)
- [ ] [\[2305.15272\] ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers](https://arxiv.org/abs/2305.15272) (HUST)
- [ ] [\[2305.15296\] MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation](https://arxiv.org/abs/2305.15296) (NIPS)
- [ ] [\[2305.15302\] Multi-Modal Mutual Attention and Iterative Interaction for Referring Image Segmentation](https://arxiv.org/abs/2305.15302) (TIP)
- [ ] [\[2305.15316\] Training on Thin Air: Improve Image Classification with Generated Data](https://arxiv.org/abs/2305.15316) (University of Toronto)
- [ ] [\[2305.15328\] Visual Programming for Text-to-Image Generation and Evaluation](https://arxiv.org/abs/2305.15328) (NIPS)
- [ ] [\[2305.15347\] A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence](https://arxiv.org/abs/2305.15347) (NIPS)
- [ ] [\[2305.15365\] Boundary Attention Mapping (BAM): Fine-grained saliency maps for segmentation of Burn Injuries](https://arxiv.org/abs/2305.15365) (University of Alberta)
- [ ] [\[2305.15393\] LayoutGPT: Compositional Visual Planning and Generation with Large Language Models](https://arxiv.org/abs/2305.15393) (NIPS)
- [ ] [\[2305.15399\] Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape](https://arxiv.org/abs/2305.15399) (ICLR)
- [ ] [\[2305.15420\] A Hybrid Semantic-Geometric Approach for Clutter-Resistant Floorplan Generation from Building Point Clouds](https://arxiv.org/abs/2305.15420) (GIT)
- [ ] [\[2305.15483\] Weakly Supervised Vision-and-Language Pre-training with Relative Representations](https://arxiv.org/abs/2305.15483) (Tsinghua)
- [ ] [\[2305.15544\] Fast Adversarial CNN-based Perturbation Attack on No-Reference Image- and Video-Quality Metrics](https://arxiv.org/abs/2305.15544) (ICLR)
- [ ] [\[2305.15560\] Differentially Private Synthetic Data via Foundation Model APIs 1: Images](https://arxiv.org/abs/2305.15560) (Microsoft)
- [ ] [\[2305.15583\] Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps](https://arxiv.org/abs/2305.15583) (ICLR)
- [ ] [\[2305.15652\] Towards Total Online Unsupervised Anomaly Detection and Localization in Industrial Vision](https://arxiv.org/abs/2305.15652) (IA CAS)
- [ ] [\[2305.15660\] Zero-shot Generation of Training Data with Denoising Diffusion Probabilistic Model for Handwritten Chinese Character Recognition](https://arxiv.org/abs/2305.15660) (USTC)
- [ ] [\[2305.15679\] A Similarity Alignment Model for Video Copy Segment Matching](https://arxiv.org/abs/2305.15679) (USTC)
- [ ] [\[2305.15700\] Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments](https://arxiv.org/abs/2305.15700) (CMU, NIPS)
- [ ] [\[2305.15701\] Action Sensitivity Learning for Temporal Action Localization](https://arxiv.org/abs/2305.15701) (ZJU, ICCV)
- [ ] [\[2305.15712\] Knowledge Diffusion for Distillation](https://arxiv.org/abs/2305.15712) (SenseTime, NIPS)
- [ ] [\[2305.15781\] VanillaKD: Revisit the Power of Vanilla Knowledge Distillation from Small Scale to Large Scale](https://arxiv.org/abs/2305.15781) (BIT)
- [ ] [\[2305.15808\] Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback](https://arxiv.org/abs/2305.15808) (HKUST(GZ))
- [ ] [\[2305.15832\] All Points Matter: Entropy-Regularized Distribution Alignment for Weakly-supervised 3D Segmentation](https://arxiv.org/abs/2305.15832) (USyd, NIPS)
- [ ] [\[2305.15836\] Improved Multi-Scale Grid Rendering of Point Clouds for Radar Object Detection Networks](https://arxiv.org/abs/2305.15836) (Bosch)
- [ ] [\[2305.15873\] Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on SE(3)](https://arxiv.org/abs/2305.15873) (CVPR)
- [ ] [\[2305.15896\] MixFormerV2: Efficient Fully Transformer Tracking](https://arxiv.org/abs/2305.15896) (NJU, NIPS)
- [ ] [\[2305.15909\] Camera-Incremental Object Re-Identification with Identity Knowledge Evolution](https://arxiv.org/abs/2305.15909) (IA CAS)
- [ ] [\[2305.15957\] DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification](https://arxiv.org/abs/2305.15957) (BIT)
- [ ] [\[2305.15975\] Triplet Knowledge Distillation](https://arxiv.org/abs/2305.15975) (ICT CAS)
- [ ] [\[2305.16025\] NVTC: Nonlinear Vector Transform Coding](https://arxiv.org/abs/2305.16025) (CVPR)
- [ ] [\[2305.16049\] CN-Celeb-AV: A Multi-Genre Audio-Visual Dataset for Person Recognition](https://arxiv.org/abs/2305.16049) (Tsinghua)
- [ ] [\[2305.16066\] Guided Attention for Next Active Object @ EGO4D STA Challenge](https://arxiv.org/abs/2305.16066) (CVPR)
- [ ] [\[2305.16124\] Robust Category-Level 3D Pose Estimation from Synthetic Data](https://arxiv.org/abs/2305.16124) (Peking)
- [ ] [\[2305.16138\] Introducing Explicit Gaze Constraints to Face Swapping](https://arxiv.org/abs/2305.16138) (UW)
- [ ] [\[2305.16214\] Self-aware and Cross-sample Prototypical Learning for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2305.16214) (Xidian)
- [ ] [\[2305.16216\] Cross-supervised Dual Classifiers for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2305.16216) (Xidian)
- [ ] [\[2305.16269\] UDPM: Upsampling Diffusion Probabilistic Models](https://arxiv.org/abs/2305.16269) (Tel Aviv)
- [ ] [\[2305.16283\] CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graph Diffusion](https://arxiv.org/abs/2305.16283) (NIPS)
- [ ] [\[2305.16295\] HAAV: Hierarchical Aggregation of Augmented Views for Image Captioning](https://arxiv.org/abs/2305.16295) (GIT, CVPR)
- [ ] [\[2305.16311\] Break-A-Scene: Extracting Multiple Concepts from a Single Image](https://arxiv.org/abs/2305.16311) (SIGGRAPH)
- [ ] [\[2305.16312\] UMat: Uncertainty-Aware Single Image High Resolution Material Capture](https://arxiv.org/abs/2305.16312) (CVPR)
- [ ] [\[2305.16322\] Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2305.16322) (HKU)
- [ ] [\[2305.16397\] Are Diffusion Models Vision-And-Language Reasoners?](https://arxiv.org/abs/2305.16397) (NIPS)
- [ ] [\[2305.16404\] GrowSP: Unsupervised Semantic Segmentation of 3D Point Clouds](https://arxiv.org/abs/2305.16404) (PolyU, CVPR)
- [ ] [\[2305.16437\] KeyPosS: Plug-and-Play Facial Landmark Detection through GPS-Inspired True-Range Multilateration](https://arxiv.org/abs/2305.16437) (CMU, ACMMM)
- [ ] [\[2305.16487\] EgoHumans: An Egocentric 3D Multi-Human Benchmark](https://arxiv.org/abs/2305.16487) (ICCV)
- [ ] [\[2305.16494\] Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability](https://arxiv.org/abs/2305.16494) (GIT, NIPS)
- [ ] [\[2305.16526\] Extending Explainable Boosting Machines to Scientific Image Data](https://arxiv.org/abs/2305.16526) (UMD)
- [ ] [\[2305.16566\] Integrating Listwise Ranking into Pairwise-based Image-Text Retrieval](https://arxiv.org/abs/2305.16566) (BUPT)
- [ ] [\[2305.16580\] TFDet: Target-Aware Fusion for RGB-T Pedestrian Detection](https://arxiv.org/abs/2305.16580) (ZJU)
- [ ] [\[2305.16687\] Balanced Supervised Contrastive Learning for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2305.16687) (KAIST)
- [ ] [\[2305.16804\] Towards Open-World Segmentation of Parts](https://arxiv.org/abs/2305.16804) (CVPR)
- [ ] [\[2305.16914\] PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Scene Reconstruction](https://arxiv.org/abs/2305.16914) (SJTU)
- [ ] [\[2305.16934\] On Evaluating Adversarial Robustness of Large Vision-Language Models](https://arxiv.org/abs/2305.16934) (Tsinghua, NIPS)
- [ ] [\[2305.16963\] Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination](https://arxiv.org/abs/2305.16963) (NIPS)
- [ ] [\[2305.16965\] Accelerating Diffusion Models for Inverse Problems through Shortcut Sampling](https://arxiv.org/abs/2305.16965) (Tsinghua)
- [ ] [\[2305.16972\] Maskomaly:Zero-Shot Mask Anomaly Segmentation](https://arxiv.org/abs/2305.16972) (ETH)
- [ ] [\[2305.16999\] Three Towers: Flexible Contrastive Learning with Pretrained Image Models](https://arxiv.org/abs/2305.16999) (Oxford, NIPS)
- [ ] [\[2305.17011\] SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation](https://arxiv.org/abs/2305.17011) (Tsinghua)
- [ ] [\[2305.17023\] Are Deep Neural Networks Adequate Behavioural Models of Human Visual Perception?](https://arxiv.org/abs/2305.17023) (University of Tübingen)
- [ ] [\[2305.17091\] SSSegmenation: An Open Source Supervised Semantic Segmentation Toolbox Based on PyTorch](https://arxiv.org/abs/2305.17091) (HKU)
- [ ] [\[2305.17098\] ControlVideo: Conditional Control for One-shot Text-driven Video Editing and Beyond](https://arxiv.org/abs/2305.17098) (Tsinghua)
- [ ] [\[2305.17102\] GeoVLN: Learning Geometry-Enhanced Visual Representation with Slot Attention for Vision-and-Language Navigation](https://arxiv.org/abs/2305.17102) (CVPR)
- [ ] [\[2305.17207\] Building One-class Detector for Anything: Open-vocabulary Zero-shot OOD Detection Using Text-image Models](https://arxiv.org/abs/2305.17207) (Google)
- [ ] [\[2305.17214\] Contrast, Attend and Diffuse to Decode High-Resolution Images from Brain Activities](https://arxiv.org/abs/2305.17214) (KU Leuven, NIPS)
- [ ] [\[2305.17219\] GVdoc: Graph-based Visual Document Classification](https://arxiv.org/abs/2305.17219) (AWS)
- [ ] [\[2305.17223\] Do We Really Need a Large Number of Visual Prompts?](https://arxiv.org/abs/2305.17223) (Yale)
- [ ] [\[2305.17235\] COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models](https://arxiv.org/abs/2305.17235) (ICML)
- [ ] [\[2305.17252\] Generalizable Pose Estimation Using Implicit Scene Representations](https://arxiv.org/abs/2305.17252) (GIT)
- [ ] [\[2305.17260\] Study of Subjective and Objective Quality Assessment of Mobile Cloud Gaming Videos](https://arxiv.org/abs/2305.17260) (UT Austin, TIP)
- [ ] [\[2305.17262\] Im-Promptu: In-Context Composition from Image Prompts](https://arxiv.org/abs/2305.17262) (Berkeley)
- [ ] [\[2305.17303\] Distilling BlackBox to Interpretable models for Efficient Transfer Learning](https://arxiv.org/abs/2305.17303) (BU)
- [ ] [\[2305.17328\] Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers](https://arxiv.org/abs/2305.17328) (CVPR)
- [ ] [\[2305.17343\] Modality-Independent Teachers Meet Weakly-Supervised Audio-Visual Event Parser](https://arxiv.org/abs/2305.17343) (NVIDIA, NIPS)
- [ ] [\[2305.17349\] Condition-Invariant Semantic Segmentation](https://arxiv.org/abs/2305.17349) (ETH)
- [ ] [\[2305.17355\] Rethinking PRL: A Multiscale Progressively Residual Learning Network for Inverse Halftoning](https://arxiv.org/abs/2305.17355) (SYSU)
- [ ] [\[2305.17398\] NeRO: Neural Geometry and BRDF Reconstruction of Reflective Objects from Multiview Images](https://arxiv.org/abs/2305.17398) (SIGGRAPH)
- [ ] [\[2305.17423\] Accelerating Text-to-Image Editing via Cache-Enabled Sparse Diffusion Inference](https://arxiv.org/abs/2305.17423) (Peking)
- [ ] [\[2305.17438\] On the Importance of Backbone to the Adversarial Robustness of Object Detectors](https://arxiv.org/abs/2305.17438) (Tsinghua)
- [ ] [\[2305.17455\] CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers](https://arxiv.org/abs/2305.17455) (Tsinghua, ICML)
- [ ] [\[2305.17489\] Text-to-image Editing by Image Information Removal](https://arxiv.org/abs/2305.17489) (CVPR)
- [ ] [\[2305.17510\] A Hybrid Quantum-Classical Approach based on the Hadamard Transform for the Convolutional Layer](https://arxiv.org/abs/2305.17510) (ICML)
- [ ] [\[2305.17555\] Diffeomorphic Mesh Deformation via Efficient Optimal Transport for Cortical Surface Reconstruction](https://arxiv.org/abs/2305.17555) (ICLR)
- [ ] [\[2305.17565\] Self-Supervised Learning of Action Affordances as Interaction Modes](https://arxiv.org/abs/2305.17565) (University of Toronto)
- [ ] [\[2305.17624\] SimpSON: Simplifying Photo Cleanup with Single-Click Distracting Object Segmentation Network](https://arxiv.org/abs/2305.17624) (CVPR)
- [ ] [\[2305.17644\] Caterpillar: A Pure-MLP Architecture with Shifted-Pillars-Concatenation](https://arxiv.org/abs/2305.17644) (UESTC)
- [ ] [\[2305.17652\] ConaCLIP: Exploring Distillation of Fully-Connected Knowledge Interaction Graph for Lightweight Text-Image Retrieval](https://arxiv.org/abs/2305.17652) (Fudan)
- [ ] [\[2305.17710\] OccCasNet: Occlusion-aware Cascade Cost Volume for Light Field Depth Estimation](https://arxiv.org/abs/2305.17710) (NUDT)
- [ ] [\[2305.17716\] InDL: A New Dataset and Benchmark for In-Diagram Logic Interpretation based on Visual Illusion](https://arxiv.org/abs/2305.17716) (University of Edinburgh)
- [ ] [\[2305.17763\] NeurOCS: Neural NOCS Supervision for Monocular 3D Object Localization](https://arxiv.org/abs/2305.17763) (CVPR)
- [ ] [\[2305.17791\] LowDINO -- A Low Parameter Self Supervised Learning Model](https://arxiv.org/abs/2305.17791) (NYU)
- [ ] [\[2305.17797\] T2FNorm: Extremely Simple Scaled Train-time Feature Normalization for OOD Detection](https://arxiv.org/abs/2305.17797) (UCL)
- [ ] [\[2305.17852\] Hierarchical Neural Memory Network for Low Latency Event Processing](https://arxiv.org/abs/2305.17852) (CVPR)
- [ ] [\[2305.17861\] Proposal-Based Multiple Instance Learning for Weakly-Supervised Temporal Action Localization](https://arxiv.org/abs/2305.17861) (USTC, CVPR)
- [ ] [\[2305.17863\] GridFormer: Residual Dense Transformer with Grid Structure for Image Restoration in Adverse Weather Conditions](https://arxiv.org/abs/2305.17863) (NJU)
- [ ] [\[2305.17891\] The Rise of AI Language Pathologists: Exploring Two-level Prompt Learning for Few-shot Weakly-supervised Whole Slide Image Classification](https://arxiv.org/abs/2305.17891) (NIPS)
- [ ] [\[2305.17940\] Learning Conditional Attributes for Compositional Zero-Shot Learning](https://arxiv.org/abs/2305.17940) (NWPU, CVPR)
- [ ] [\[2305.17972\] View-to-Label: Multi-View Consistency for Self-Supervised 3D Object Detection](https://arxiv.org/abs/2305.17972) (TUM)
- [ ] [\[2305.17975\] Jigsaw: Learning to Assemble Multiple Fractured Objects](https://arxiv.org/abs/2305.17975) (UT Austin, NIPS)
- [ ] [\[2305.17997\] DiffRate : Differentiable Compression Rate for Efficient Vision Transformers](https://arxiv.org/abs/2305.17997) (Xiamen)
- [ ] [\[2305.18007\] Conditional Score Guidance for Text-Driven Image-to-Image Translation](https://arxiv.org/abs/2305.18007) (NIPS)
- [ ] [\[2305.18010\] Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/abs/2305.18010) (ICLR)
- [ ] [\[2305.18063\] Vector-based Representation is the Key: A Study on Disentanglement and Compositional Generalization](https://arxiv.org/abs/2305.18063) (XJTU)
- [ ] [\[2305.18072\] Image Captioning with Multi-Context Synthetic Data](https://arxiv.org/abs/2305.18072) (USTC)
- [ ] [\[2305.18107\] Crafting Training Degradation Distribution for the Accuracy-Generalization Trade-off in Real-World Super-Resolution](https://arxiv.org/abs/2305.18107) (USyd, ICML)
- [ ] [\[2305.18135\] Alignment-free HDR Deghosting with Semantics Consistent Transformer](https://arxiv.org/abs/2305.18135) (ICCV)
- [ ] [\[2305.18158\] Out-of-Distributed Semantic Pruning for Robust Semi-Supervised Learning](https://arxiv.org/abs/2305.18158) (CVPR)
- [ ] [\[2305.18163\] Compact Real-time Radiance Fields with Neural Codebook](https://arxiv.org/abs/2305.18163) (HKUST)
- [ ] [\[2305.18171\] Improved Probabilistic Image-Text Representations](https://arxiv.org/abs/2305.18171) (ICLR)
- [ ] [\[2305.18259\] GlyphControl: Glyph Conditional Control for Visual Text Generation](https://arxiv.org/abs/2305.18259) (Microsoft, NIPS)
- [ ] [\[2305.18264\] Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising](https://arxiv.org/abs/2305.18264) (Tsinghua)
- [ ] [\[2305.18274\] Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors](https://arxiv.org/abs/2305.18274) (NIPS)
- [ ] [\[2305.18279\] Contextual Object Detection with Multimodal Large Language Models](https://arxiv.org/abs/2305.18279) (NTU)
- [ ] [\[2305.18287\] LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections](https://arxiv.org/abs/2305.18287) (NIPS)
- [ ] [\[2305.18295\] RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths](https://arxiv.org/abs/2305.18295) (HKU, NIPS)
- [ ] [\[2305.18326\] BigVideo: A Large-scale Video Subtitle Translation Dataset for Multimodal Machine Translation](https://arxiv.org/abs/2305.18326) (Xiamen)
- [ ] [\[2305.18337\] You Don't Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images](https://arxiv.org/abs/2305.18337) (Imperial)
- [ ] [\[2305.18373\] KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models](https://arxiv.org/abs/2305.18373) (UCSD)
- [ ] [\[2305.18499\] Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning](https://arxiv.org/abs/2305.18499) (Tsinghua, NIPS)
- [ ] [\[2305.18500\] VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset](https://arxiv.org/abs/2305.18500) (NIPS)
- [ ] [\[2305.18547\] Learning from Multi-Perception Features for Real-Word Image Super-resolution](https://arxiv.org/abs/2305.18547) (NWPU)
- [ ] [\[2305.18565\] PaLI-X: On Scaling up a Multilingual Vision and Language Model](https://arxiv.org/abs/2305.18565) (Google)
- [ ] [\[2305.18712\] Can We Evaluate Domain Adaptation Models Without Target-Domain Labels?](https://arxiv.org/abs/2305.18712) (NTU, ICLR)
- [ ] [\[2305.18714\] Align, Perturb and Decouple: Toward Better Leverage of Difference Information for RSI Change Detection](https://arxiv.org/abs/2305.18714) (Fudan)
- [ ] [\[2305.18723\] Towards Accurate Post-training Quantization for Diffusion Models](https://arxiv.org/abs/2305.18723) (CMU)
- [ ] [\[2305.18743\] Decomposed Human Motion Prior for Video Pose Estimation via Adversarial Training](https://arxiv.org/abs/2305.18743) (Tsinghua)
- [ ] [\[2305.18769\] DualVAE: Controlling Colours of Generated and Real Images](https://arxiv.org/abs/2305.18769) (Google)
- [ ] [\[2305.18812\] DiffSketching: Sketch Control Image Synthesis with Diffusion Models](https://arxiv.org/abs/2305.18812) (BUPT)
- [ ] [\[2305.18830\] Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions](https://arxiv.org/abs/2305.18830) (UESTC)
- [ ] [\[2305.18832\] ReTR: Modeling Rendering Via Transformer for Generalizable Neural Surface Reconstruction](https://arxiv.org/abs/2305.18832) (HKUST(GZ))
- [ ] [\[2305.18878\] BPF Algorithms for Multiple Source-Translation Computed Tomography Reconstruction](https://arxiv.org/abs/2305.18878) (Chongqing)
- [ ] [\[2305.18891\] EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture Generation](https://arxiv.org/abs/2305.18891) (Queensland)
- [ ] [\[2305.18947\] A Probabilistic Rotation Representation for Symmetric Shapes With an Efficiently Computable Bingham Loss Function](https://arxiv.org/abs/2305.18947) (University of Tokyo)
- [ ] [\[2305.18948\] Prompt-Based Tuning of Transformer Models for Multi-Center Medical Image Segmentation of Head and Neck Cancer](https://arxiv.org/abs/2305.18948) (MBZUAI)
- [ ] [\[2305.18969\] MS-DETR: Natural Language Video Localization with Sampling Moment-Moment Interaction](https://arxiv.org/abs/2305.18969) (A*STAR,)
- [ ] [\[2305.18993\] ConES: Concept Embedding Search for Parameter Efficient Tuning Large Vision Language Models](https://arxiv.org/abs/2305.18993) (BUPT)
- [ ] [\[2305.19084\] Joint Optimization of Class-Specific Training- and Test-Time Data Augmentation in Segmentation](https://arxiv.org/abs/2305.19084) (Imperial)
- [ ] [\[2305.19094\] Diffusion Model for Dense Matching](https://arxiv.org/abs/2305.19094) (ICLR)
- [ ] [\[2305.19124\] Calliffusion: Chinese Calligraphy Generation and Style Transfer with Diffusion Modeling](https://arxiv.org/abs/2305.19124) (MBZUAI)
- [ ] [\[2305.19164\] LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images](https://arxiv.org/abs/2305.19164) (NIPS)
- [ ] [\[2305.19201\] DaRF: Boosting Radiance Fields from Sparse Inputs with Monocular Depth Adaptation](https://arxiv.org/abs/2305.19201) (NIPS)
- [ ] [\[2305.19270\] Learning without Forgetting for Vision-Language Models](https://arxiv.org/abs/2305.19270) (NJU)
- [ ] [\[2305.19302\] Smooth, exact rotational symmetrization for deep learning on point clouds](https://arxiv.org/abs/2305.19302) (EPFL)
- [ ] [\[2305.19374\] Compositional diversity in visual concept learning](https://arxiv.org/abs/2305.19374) (NYU)
- [ ] [\[2305.19412\] Are Large Kernels Better Teachers than Transformers for ConvNets?](https://arxiv.org/abs/2305.19412) (UT Austin, ICML)
- [ ] [\[2305.19480\] Learning by Aligning 2D Skeleton Sequences and Multi-Modality Fusion](https://arxiv.org/abs/2305.19480) (ECCV)
- [ ] [\[2305.19486\] Instance-dependent Noisy-label Learning with Graphical Model Based Noise-rate Estimation](https://arxiv.org/abs/2305.19486) (ECCV)
- [ ] [\[2305.19492\] CVSNet: A Computer Implementation for Central Visual System of The Brain](https://arxiv.org/abs/2305.19492) (UESTC)
- [ ] [\[2305.19507\] Manifold Constraint Regularization for Remote Sensing Image Generation](https://arxiv.org/abs/2305.19507) (Tsinghua)
- [ ] [\[2305.19543\] Improving Handwritten OCR with Training Samples Generated by Glyph Conditional Denoising Diffusion Probabilistic Model](https://arxiv.org/abs/2305.19543) (USTC)
- [ ] [\[2305.19547\] Inferring and Leveraging Parts from Object Shape for Improving Semantic Image Synthesis](https://arxiv.org/abs/2305.19547) (PolyU, CVPR)
- [ ] [\[2305.19550\] Spotlight Attention: Robust Object-Centric Learning With a Spatial Locality Prior](https://arxiv.org/abs/2305.19550) (NIPS)
- [ ] [\[2305.19590\] Neural Kernel Surface Reconstruction](https://arxiv.org/abs/2305.19590) (CVPR)
- [ ] [\[2305.19643\] Mask, Stitch, and Re-Sample: Enhancing Robustness and Generalizability in Anomaly Detection through Automatic Diffusion Models](https://arxiv.org/abs/2305.19643) (TUM)
- [ ] [\[2305.19700\] GaitGS: Temporal Feature Learning in Granularity and Span Dimension for Gait Recognition](https://arxiv.org/abs/2305.19700) (HUST)
- [ ] [\[2305.19787\] DeepMerge: Deep-Learning-Based Region-Merging for Image Segmentation](https://arxiv.org/abs/2305.19787) (WHU)
- [ ] [\[2305.19809\] Direct Diffusion Bridge using Data Consistency for Inverse Problems](https://arxiv.org/abs/2305.19809) (NIPS)
- [ ] [\[2305.19812\] A Survey of Label-Efficient Deep Learning for 3D Point Clouds](https://arxiv.org/abs/2305.19812) (TPAMI)
- [ ] [\[2305.19862\] Self-supervised Learning to Bring Dual Reversed Rolling Shutter Images Alive](https://arxiv.org/abs/2305.19862) (ICCV)
- [ ] [\[2305.19949\] Treasure in Distribution: A Domain Randomization based Multi-Source Domain Generalization for 2D Medical Image Segmentation](https://arxiv.org/abs/2305.19949) (NWPU)
- [ ] [\[2305.19957\] DeepSolo++: Let Transformer Decoder with Explicit Points Solo for Multilingual Text Spotting](https://arxiv.org/abs/2305.19957) (WHU, CVPR)
- [ ] [\[2305.20049\] A Unified Conditional Framework for Diffusion-based Image Restoration](https://arxiv.org/abs/2305.20049) (CUHK)
- [ ] [\[2305.20062\] Chatting Makes Perfect: Chat-based Image Retrieval](https://arxiv.org/abs/2305.20062) (NIPS)
- [ ] [\[2305.20087\] Too Large; Data Reduction for Vision-Language Pre-Training](https://arxiv.org/abs/2305.20087) (ICCV)
- [ ] [\[2305.20088\] Improving CLIP Training with Language Rewrites](https://arxiv.org/abs/2305.20088) (NIPS)
- [ ] [\[2305.20089\] Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images](https://arxiv.org/abs/2305.20089) (IA CAS)
- [ ] [\[2305.20091\] Humans in 4D: Reconstructing and Tracking Humans with Transformers](https://arxiv.org/abs/2305.20091) (Berkeley, ICCV)
- [ ] [\[2306.00001\] TinyissimoYOLO: A Quantized, Low-Memory Footprint, TinyML Object Detection Network for Low Power Microcontrollers](https://arxiv.org/abs/2306.00001) (ETH)
- [ ] [\[2306.00150\] Enrichment of the NLST and NSCLC-Radiomics computed tomography collections with AI-derived annotations](https://arxiv.org/abs/2306.00150) (Harvard)
- [ ] [\[2306.00200\] Zero-shot Pose Transfer for Unrigged Stylized 3D Characters](https://arxiv.org/abs/2306.00200) (CVPR)
- [ ] [\[2306.00241\] Balancing Reconstruction and Editing Quality of GAN Inversion for Real Image Editing with StyleGAN Prior Latent Space](https://arxiv.org/abs/2306.00241) (University of Tokyo)
- [ ] [\[2306.00306\] Low-Light Image Enhancement with Wavelet-based Diffusion Models](https://arxiv.org/abs/2306.00306) (SIGGRAPH)
- [ ] [\[2306.00310\] Prompt Algebra for Task Composition](https://arxiv.org/abs/2306.00310) (AWS)
- [ ] [\[2306.00349\] CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception](https://arxiv.org/abs/2306.00349) (University of Michigan)
- [ ] [\[2306.00354\] Addressing Negative Transfer in Diffusion Models](https://arxiv.org/abs/2306.00354) (EPFL, NIPS)
- [ ] [\[2306.00360\] How Do ConvNets Understand Image Intensity?](https://arxiv.org/abs/2306.00360) (University of Toronto)
- [ ] [\[2306.00370\] Graph Switching Dynamical Systems](https://arxiv.org/abs/2306.00370) (UVA.NL, ICML)
- [ ] [\[2306.00379\] Large Scale Generative Multimodal Attribute Extraction for E-commerce Attributes](https://arxiv.org/abs/2306.00379) (AWS)
- [ ] [\[2306.00386\] Symmetric Uncertainty-Aware Feature Transmission for Depth Super-Resolution](https://arxiv.org/abs/2306.00386) (WHU, ACMMM)
- [ ] [\[2306.00396\] Lightweight Vision Transformer with Bidirectional Interaction](https://arxiv.org/abs/2306.00396) (Tsinghua)
- [ ] [\[2306.00402\] Discriminative Deep Feature Visualization for Explainable Face Recognition](https://arxiv.org/abs/2306.00402) (EPFL)
- [ ] [\[2306.00407\] Towards Interactive Image Inpainting via Sketch Refinement](https://arxiv.org/abs/2306.00407) (USTC)
- [ ] [\[2306.00416\] Interactive Character Control with Auto-Regressive Motion Diffusion Models](https://arxiv.org/abs/2306.00416) (Shanghai AI Lab)
- [ ] [\[2306.00440\] Edge-guided Representation Learning for Underwater Object Detection](https://arxiv.org/abs/2306.00440) (Peking)
- [ ] [\[2306.00519\] DiffInDScene: Diffusion-based High-Quality 3D Indoor Scene Generation](https://arxiv.org/abs/2306.00519) (CUHK)
- [ ] [\[2306.00547\] AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars](https://arxiv.org/abs/2306.00547) (MPI)
- [ ] [\[2306.00576\] MammalNet: A Large-scale Video Benchmark for Mammal Recognition and Behavior Understanding](https://arxiv.org/abs/2306.00576) (CVPR)
- [ ] [\[2306.00579\] FMapping: Factorized Efficient Neural Field Mapping for Real-Time Dense RGB SLAM](https://arxiv.org/abs/2306.00579) (HKUST(GZ))
- [ ] [\[2306.00595\] Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective](https://arxiv.org/abs/2306.00595) (NIPS)
- [ ] [\[2306.00612\] AD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud Dataset](https://arxiv.org/abs/2306.00612) (Fudan, NIPS)
- [ ] [\[2306.00658\] NeuroGF: A Neural Representation for Fast Geodesic Distance and Path Queries](https://arxiv.org/abs/2306.00658) (NTU, NIPS)
- [ ] [\[2306.00693\] GPT4Image: Can Large Pre-trained Models Help Vision Models on Perception Tasks?](https://arxiv.org/abs/2306.00693) (Peking)
- [ ] [\[2306.00704\] DAM-Net: Global Flood Detection from SAR Imagery Using Differential Attention Metric-Based Vision Transformers](https://arxiv.org/abs/2306.00704) (WHU)
- [ ] [\[2306.00777\] Object pop-up: Can we infer 3D objects and their poses from human interactions alone?](https://arxiv.org/abs/2306.00777) (University of Tübingen, CVPR)
- [ ] [\[2306.00783\] FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models](https://arxiv.org/abs/2306.00783) (HKUST)
- [ ] [\[2306.00800\] FigGen: Text to Scientific Figure Generation](https://arxiv.org/abs/2306.00800) (ICLR)
- [ ] [\[2306.00863\] DeepFake-Adapter: Dual-Level Adapter for DeepFake Detection](https://arxiv.org/abs/2306.00863) (HIT)
- [ ] [\[2306.00906\] MOSAIC: Masked Optimisation with Selective Attention for Image Reconstruction](https://arxiv.org/abs/2306.00906) (Harvard)
- [ ] [\[2306.00917\] Vocabulary-free Image Classification](https://arxiv.org/abs/2306.00917) (NIPS)
- [ ] [\[2306.00950\] Differential Diffusion: Giving Each Pixel Its Strength](https://arxiv.org/abs/2306.00950) (Tel Aviv)
- [ ] [\[2306.00956\] The ObjectFolder Benchmark: Multisensory Learning with Neural and Real Objects](https://arxiv.org/abs/2306.00956) (CVPR)
- [ ] [\[2306.00965\] BUOL: A Bottom-Up Framework with Occupancy-aware Lifting for Panoptic 3D Scene Reconstruction From A Single Image](https://arxiv.org/abs/2306.00965) (CVPR)
- [ ] [\[2306.00968\] GRES: Generalized Referring Expression Segmentation](https://arxiv.org/abs/2306.00968) (CVPR)
- [ ] [\[2306.00971\] ViCo: Plug-and-play Visual Condition for Personalized Text-to-image Generation](https://arxiv.org/abs/2306.00971) (HKU)
- [ ] [\[2306.00973\] Intelligent Grimm -- Open-ended Visual Storytelling via Latent Diffusion Models](https://arxiv.org/abs/2306.00973) (CVPR)
- [ ] [\[2306.00977\] AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation](https://arxiv.org/abs/2306.00977) (ICLR)
- [ ] [\[2306.00979\] Building Rearticulable Models for Arbitrary 3D Objects from 4D Point Clouds](https://arxiv.org/abs/2306.00979) (CVPR)
- [ ] [\[2306.00989\] Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles](https://arxiv.org/abs/2306.00989) (Meta, ICML)
- [ ] [\[2306.01075\] Pedestrian Crossing Action Recognition and Trajectory Prediction with 3D Human Keypoints](https://arxiv.org/abs/2306.01075) (Stanford)
- [ ] [\[2306.01111\] Exploring the Versatility of Zero-Shot CLIP for Interstitial Lung Disease Classification](https://arxiv.org/abs/2306.01111) (Stanford)
- [ ] [\[2306.01195\] Consistency-guided Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2306.01195) (ICLR)
- [ ] [\[2306.01268\] DeepScribe: Localization and Classification of Elamite Cuneiform Signs Via Deep Learning](https://arxiv.org/abs/2306.01268) (Columbia University)
- [ ] [\[2306.01293\] LoCoOp: Few-Shot Out-of-Distribution Detection via Prompt Learning](https://arxiv.org/abs/2306.01293) (NIPS)
- [ ] [\[2306.01340\] Transformer-based Annotation Bias-aware Medical Image Segmentation](https://arxiv.org/abs/2306.01340) (NWPU)
- [ ] [\[2306.01405\] Learning Signed Distance Functions from Noisy 3D Point Clouds via Noise to Noise Mapping](https://arxiv.org/abs/2306.01405) (Tsinghua, ICML)
- [ ] [\[2306.01438\] Bi-LRFusion: Bi-Directional LiDAR-Radar Fusion for 3D Dynamic Object Detection](https://arxiv.org/abs/2306.01438) (CVPR)
- [ ] [\[2306.01452\] dugMatting: Decomposed-Uncertainty-Guided Matting](https://arxiv.org/abs/2306.01452) (Tianjin)
- [ ] [\[2306.01461\] PolyDiffuse: Polygonal Shape Reconstruction via Guided Set Diffusion Models](https://arxiv.org/abs/2306.01461) (NIPS)
- [ ] [\[2306.01500\] A Feature Reuse Framework with Texture-adaptive Aggregation for Reference-based Super-Resolution](https://arxiv.org/abs/2306.01500) (ZJU)
- [ ] [\[2306.01531\] PanoGRF: Generalizable Spherical Radiance Fields for Wide-baseline Panoramas](https://arxiv.org/abs/2306.01531) (Tsinghua, NIPS)
- [ ] [\[2306.01567\] Segment Anything in High Quality](https://arxiv.org/abs/2306.01567) (NIPS)
- [ ] [\[2306.01736\] DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model](https://arxiv.org/abs/2306.01736) (Google)
- [ ] [\[2306.01738\] OCBEV: Object-Centric BEV Transformer for Multi-View 3D Object Detection](https://arxiv.org/abs/2306.01738) (Shanghai AI Lab)
- [ ] [\[2306.01756\] CSI-Based Efficient Self-Quarantine Monitoring System Using Branchy Convolution Neural Network](https://arxiv.org/abs/2306.01756) (PolyU)
- [ ] [\[2306.01851\] Open-world Text-specified Object Counting](https://arxiv.org/abs/2306.01851) (Oxford)
- [ ] [\[2306.01879\] Revisiting the Role of Language Priors in Vision-Language Models](https://arxiv.org/abs/2306.01879) (CMU, ICML)
- [ ] [\[2306.01902\] Unlearnable Examples for Diffusion Models: Protect Data from Unauthorized Exploitation](https://arxiv.org/abs/2306.01902) (UCAS)
- [ ] [\[2306.01904\] Overcoming the Stability Gap in Continual Learning](https://arxiv.org/abs/2306.01904) (Rochester Institute of Technology)
- [ ] [\[2306.01923\] The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation](https://arxiv.org/abs/2306.01923) (NIPS)
- [ ] [\[2306.02000\] Context-PIPs: Persistent Independent Particles Demands Spatial Context Features](https://arxiv.org/abs/2306.02000) (CUHK)
- [ ] [\[2306.02014\] Uncovering the Hidden Dynamics of Video Self-supervised Learning under Distribution Shifts](https://arxiv.org/abs/2306.02014) (Google, NIPS)
- [ ] [\[2306.02061\] Balancing Logit Variation for Long-tailed Semantic Segmentation](https://arxiv.org/abs/2306.02061) (IA CAS)
- [ ] [\[2306.02080\] Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models](https://arxiv.org/abs/2306.02080) (Oxford, NIPS)
- [ ] [\[2306.02095\] Content-aware Token Sharing for Efficient Semantic Segmentation with Vision Transformers](https://arxiv.org/abs/2306.02095) (CVPR)
- [ ] [\[2306.02098\] Large, Complex, and Realistic Safety Clothing and Helmet Detection: Dataset and Method](https://arxiv.org/abs/2306.02098) (HUST)
- [ ] [\[2306.02099\] Enhancing Surface Neural Implicits with Curvature-Guided Sampling and Uncertainty-Augmented Representations](https://arxiv.org/abs/2306.02099) (TUM)
- [ ] [\[2306.02240\] ProTeCt: Prompt Tuning for Taxonomic Open Set Classification](https://arxiv.org/abs/2306.02240) (CVPR)
- [ ] [\[2306.02243\] Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification](https://arxiv.org/abs/2306.02243) (ZJU)
- [ ] [\[2306.02291\] 3rd Place Solution for PVUW2023 VSS Track: A Large Model for Semantic Segmentation on VSPW](https://arxiv.org/abs/2306.02291) (CVPR)
- [ ] [\[2306.02301\] rPPG-MAE: Self-supervised Pre-training with Masked Autoencoders for Remote Physiological Measurement](https://arxiv.org/abs/2306.02301) (BU)
- [ ] [\[2306.02416\] Training Like a Medical Resident: Context-Prior Learning Toward Universal Medical Image Segmentation](https://arxiv.org/abs/2306.02416) (CVPR)
- [ ] [\[2306.02500\] Systematic Visual Reasoning through Object-Centric Relational Abstraction](https://arxiv.org/abs/2306.02500) (Princeton)
- [ ] [\[2306.02558\] Multi-View Representation is What You Need for Point-Cloud Pre-Training](https://arxiv.org/abs/2306.02558) (ICLR)
- [ ] [\[2306.02562\] Video Diffusion Models with Local-Global Context Guidance](https://arxiv.org/abs/2306.02562) (Tsinghua)
- [ ] [\[2306.02583\] Stable Diffusion is Unstable](https://arxiv.org/abs/2306.02583) (USyd)
- [ ] [\[2306.02589\] DAGrid: Directed Accumulator Grid](https://arxiv.org/abs/2306.02589) (Cornell)
- [ ] [\[2306.02602\] ReContrast: Domain-Specific Anomaly Detection via Contrastive Reconstruction](https://arxiv.org/abs/2306.02602) (NIPS)
- [ ] [\[2306.02651\] Dynamic Interactive Relation Capturing via Scene Graph Learning for Robotic Surgical Report Generation](https://arxiv.org/abs/2306.02651) (HKUST(GZ))
- [ ] [\[2306.02691\] Cyclic Learning: Bridging Image-level Labels and Nuclei Instance Segmentation](https://arxiv.org/abs/2306.02691) (ZJU)
- [ ] [\[2306.02712\] NFTVis: Visual Analysis of NFT Performance](https://arxiv.org/abs/2306.02712) (ZJU)
- [ ] [\[2306.02760\] A2B: Anchor to Barycentric Coordinate for Robust Correspondence](https://arxiv.org/abs/2306.02760) (HUST)
- [ ] [\[2306.02763\] STAR Loss: Reducing Semantic Ambiguity in Facial Landmark Detection](https://arxiv.org/abs/2306.02763) (Xiamen, CVPR)
- [ ] [\[2306.02776\] Cheap-fake Detection with LLM using Prompt Engineering](https://arxiv.org/abs/2306.02776) (SJTU)
- [ ] [\[2306.02850\] TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D Environments](https://arxiv.org/abs/2306.02850) (MPI)
- [ ] [\[2306.02898\] Towards Unified Text-based Person Retrieval: A Large-scale Multi-Attribute and Language Search Benchmark](https://arxiv.org/abs/2306.02898) (XJTU)
- [ ] [\[2306.02903\] Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions](https://arxiv.org/abs/2306.02903) (SJTU)
- [ ] [\[2306.02949\] INDigo: An INN-Guided Probabilistic Diffusion Algorithm for Inverse Problems](https://arxiv.org/abs/2306.02949) (Imperial)
- [ ] [\[2306.02956\] Explicit Neural Surfaces: Learning Continuous Geometry With Deformation Fields](https://arxiv.org/abs/2306.02956) (University of Edinburgh)
- [ ] [\[2306.03022\] Interpretable Alzheimer's Disease Classification Via a Contrastive Diffusion Autoencoder](https://arxiv.org/abs/2306.03022) (UCL)
- [ ] [\[2306.03066\] Of Mice and Mates: Automated Classification and Modelling of Mouse Behaviour in Groups using a Single Model across Cages](https://arxiv.org/abs/2306.03066) (University of Edinburgh)
- [ ] [\[2306.03089\] Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models](https://arxiv.org/abs/2306.03089) (CMU, NIPS)
- [ ] [\[2306.03092\] Neuralangelo: High-Fidelity Neural Surface Reconstruction](https://arxiv.org/abs/2306.03092) (CVPR)
- [ ] [\[2306.03206\] MoDAR: Using Motion Forecasting for 3D Object Detection in Point Cloud Sequences](https://arxiv.org/abs/2306.03206) (CVPR)
- [ ] [\[2306.03374\] PGformer: Proxy-Bridged Game Transformer for Multi-Person Highly Interactive Extreme Motion Prediction](https://arxiv.org/abs/2306.03374) (HKU)
- [ ] [\[2306.03403\] SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation](https://arxiv.org/abs/2306.03403) (ZJU)
- [ ] [\[2306.03413\] DVIS: Decoupled Video Instance Segmentation Framework](https://arxiv.org/abs/2306.03413) (ICCV)
- [ ] [\[2306.03414\] DreamSparse: Escaping from Plato's Cave with 2D Frozen Diffusion Model Given Sparse Views](https://arxiv.org/abs/2306.03414) (University of Tokyo)
- [ ] [\[2306.03422\] Prompting Large Language Models to Reformulate Queries for Moment Localization](https://arxiv.org/abs/2306.03422) (Fudan)
- [ ] [\[2306.03428\] GaitGCI: Generative Counterfactual Intervention for Gait Recognition](https://arxiv.org/abs/2306.03428) (CVPR)
- [ ] [\[2306.03430\] Revisiting the Trade-off between Accuracy and Robustness via Weight Distribution of Filters](https://arxiv.org/abs/2306.03430) (TPAMI)
- [ ] [\[2306.03437\] DFormer: Diffusion-guided Transformer for Universal Image Segmentation](https://arxiv.org/abs/2306.03437) (Chongqing)
- [ ] [\[2306.03445\] MetaGait: Learning to Learn an Omni Sample Adaptive Representation for Gait Recognition](https://arxiv.org/abs/2306.03445) (ECCV)
- [ ] [\[2306.03504\] Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis](https://arxiv.org/abs/2306.03504) (ZJU)
- [ ] [\[2306.03508\] Semantic Segmentation on VSPW Dataset through Contrastive Loss and Multi-dataset Training Approach](https://arxiv.org/abs/2306.03508) (XJTU, CVPR)
- [ ] [\[2306.03538\] SDR-GAIN: A High Real-Time Occluded Pedestrian Pose Completion Method for Autonomous Driving](https://arxiv.org/abs/2306.03538) (NTU)
- [ ] [\[2306.03576\] Human 3D Avatar Modeling with Implicit Neural Representation: A Brief Survey](https://arxiv.org/abs/2306.03576) (Fudan)
- [ ] [\[2306.03584\] RDFC-GAN: RGB-Depth Fusion CycleGAN for Indoor Depth Completion](https://arxiv.org/abs/2306.03584) (BUPT, CVPR)
- [ ] [\[2306.03594\] Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks](https://arxiv.org/abs/2306.03594) (HKUST(GZ))
- [ ] [\[2306.03597\] Human-Object Interaction Prediction in Videos through Gaze Following](https://arxiv.org/abs/2306.03597) (TUM)
- [ ] [\[2306.03630\] Mutual Information Regularization for Weakly-supervised RGB-D Salient Object Detection](https://arxiv.org/abs/2306.03630) (NWPU)
- [ ] [\[2306.03679\] Human-imperceptible, Machine-recognizable Images](https://arxiv.org/abs/2306.03679) (University of Edinburgh)
- [ ] [\[2306.03711\] Deep Learning-Enabled Sleep Staging From Vital Signs and Activity Measured Using a Near-Infrared Video Camera](https://arxiv.org/abs/2306.03711) (Oxford)
- [ ] [\[2306.03747\] Towards Scalable Multi-View Reconstruction of Geometry and Materials](https://arxiv.org/abs/2306.03747) (MPI)
- [ ] [\[2306.03810\] X-Align++: cross-modal cross-view alignment for Bird's-eye-view segmentation](https://arxiv.org/abs/2306.03810) (UCSD)
- [ ] [\[2306.03847\] Learning Human Mesh Recovery in 3D Scenes](https://arxiv.org/abs/2306.03847) (CVPR)
- [ ] [\[2306.03881\] Emergent Correspondence from Image Diffusion](https://arxiv.org/abs/2306.03881) (NIPS)
- [ ] [\[2306.03899\] Towards Label-free Scene Understanding by Vision Foundation Models](https://arxiv.org/abs/2306.03899) (NIPS)
- [ ] [\[2306.03932\] Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!](https://arxiv.org/abs/2306.03932) (CVPR)
- [ ] [\[2306.04021\] Energy-Based Models for Cross-Modal Localization using Convolutional Transformers](https://arxiv.org/abs/2306.04021) (MIT)
- [ ] [\[2306.04047\] CAVEN: An Embodied Conversational Agent for Efficient Audio-Visual Navigation in Noisy Environments](https://arxiv.org/abs/2306.04047) (UW)
- [ ] [\[2306.04121\] Matte Anything: Interactive Natural Image Matting with Segment Anything Models](https://arxiv.org/abs/2306.04121) (HUST)
- [ ] [\[2306.04147\] CFDP: Common Frequency Domain Pruning](https://arxiv.org/abs/2306.04147) (CVPR)
- [ ] [\[2306.04166\] BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives](https://arxiv.org/abs/2306.04166) (UCSD)
- [ ] [\[2306.04216\] MMSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos](https://arxiv.org/abs/2306.04216) (MIT)
- [ ] [\[2306.04231\] Learning Probabilistic Coordinate Fields for Robust Correspondences](https://arxiv.org/abs/2306.04231) (TPAMI)
- [ ] [\[2306.04236\] Flare7K++: Mixing Synthetic and Real Datasets for Nighttime Flare Removal and Beyond](https://arxiv.org/abs/2306.04236) (NTU)
- [ ] [\[2306.04240\] T-ADAF: Adaptive Data Augmentation Framework for Image Classification Network based on Tensor T-product Operator](https://arxiv.org/abs/2306.04240) (Fudan)
- [ ] [\[2306.04244\] Coarse Is Better? A New Pipeline Towards Self-Supervised Learning with Uncurated Images](https://arxiv.org/abs/2306.04244) (NJU)
- [ ] [\[2306.04272\] On the Generalization of Multi-modal Contrastive Learning](https://arxiv.org/abs/2306.04272) (Peking)
- [ ] [\[2306.04344\] ViDA: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation](https://arxiv.org/abs/2306.04344) (Peking, ICLR)
- [ ] [\[2306.04345\] An Overview of Challenges in Egocentric Text-Video Retrieval](https://arxiv.org/abs/2306.04345) (NTU)
- [ ] [\[2306.04356\] Fine-Grained Visual Prompting](https://arxiv.org/abs/2306.04356) (Nankai)
- [ ] [\[2306.04385\] SF-FSDA: Source-Free Few-Shot Domain Adaptive Object Detection with Efficient Labeled Data Factory](https://arxiv.org/abs/2306.04385) (EPFL)
- [ ] [\[2306.04387\] M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning](https://arxiv.org/abs/2306.04387) (Peking)
- [ ] [\[2306.04396\] Improving Diffusion-based Image Translation using Asymmetric Gradient Guidance](https://arxiv.org/abs/2306.04396) (KAIST)
- [ ] [\[2306.04482\] ICON$^2$: Reliably Benchmarking Predictive Inequity in Object Detection](https://arxiv.org/abs/2306.04482) (Columbia University)
- [ ] [\[2306.04506\] Defocus to focus: Photo-realistic bokeh rendering by fusing defocus and radiance priors](https://arxiv.org/abs/2306.04506) (HUST)
- [ ] [\[2306.04507\] Improving neural network representations using human similarity judgments](https://arxiv.org/abs/2306.04507) (NIPS)
- [ ] [\[2306.04557\] PhenoBench -- A Large Dataset and Benchmarks for Semantic Image Interpretation in the Agricultural Domain](https://arxiv.org/abs/2306.04557) (Oxford, TPAMI)
- [ ] [\[2306.04607\] GeoDiffusion: Text-Prompted Geometric Control for Object Detection Data Generation](https://arxiv.org/abs/2306.04607) (HKUST, ICLR)
- [ ] [\[2306.04633\] Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion](https://arxiv.org/abs/2306.04633) (NIPS)
- [ ] [\[2306.04636\] GP-UNIT: Generative Prior for Versatile Unsupervised Image-to-Image Translation](https://arxiv.org/abs/2306.04636) (TPAMI)
- [ ] [\[2306.04641\] Generalizable Low-Resource Activity Recognition with Diverse and Discriminative Representation Learning](https://arxiv.org/abs/2306.04641) (ICT CAS)
- [ ] [\[2306.04650\] GaitMPL: Gait Recognition with Memory-Augmented Progressive Learning](https://arxiv.org/abs/2306.04650) (ZJU, TIP)
- [ ] [\[2306.04652\] Language Adaptive Weight Generation for Multi-task Visual Grounding](https://arxiv.org/abs/2306.04652) (ZJU, CVPR)
- [ ] [\[2306.04699\] DiViNeT: 3D Reconstruction from Disparate Views via Neural Template Regularization](https://arxiv.org/abs/2306.04699) (NIPS)
- [ ] [\[2306.04715\] UniBoost: Unsupervised Unimodal Pre-training for Boosting Zero-shot Vision-Language Tasks](https://arxiv.org/abs/2306.04715) (Tsinghua)
- [ ] [\[2306.04717\] AGIQA-3K: An Open Database for AI-Generated Image Quality Assessment](https://arxiv.org/abs/2306.04717) (SJTU)
- [ ] [\[2306.04719\] Don't trust your eyes: on the (un)reliability of feature visualizations](https://arxiv.org/abs/2306.04719) (Google, ICML)
- [ ] [\[2306.04744\] WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models](https://arxiv.org/abs/2306.04744) (CVPR)
- [ ] [\[2306.04745\] 3D Human Keypoints Estimation From Point Clouds in the Wild Without Human Labels](https://arxiv.org/abs/2306.04745) (CVPR)
- [ ] [\[2306.04811\] Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation](https://arxiv.org/abs/2306.04811) (USTC)
- [ ] [\[2306.04829\] Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities](https://arxiv.org/abs/2306.04829) (MPI, NIPS)
- [ ] [\[2306.04849\] ScaleDet: A Scalable Multi-Dataset Object Detector](https://arxiv.org/abs/2306.04849) (CVPR)
- [ ] [\[2306.04889\] ShaDDR: Interactive Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering](https://arxiv.org/abs/2306.04889) (SIGGRAPH)
- [ ] [\[2306.04911\] Test-Time Style Shifting: Handling Arbitrary Styles in Domain Generalization](https://arxiv.org/abs/2306.04911) (ICML)
- [ ] [\[2306.04927\] An Efficient Transformer for Simultaneous Learning of BEV and Lane Representations in 3D Lane Detection](https://arxiv.org/abs/2306.04927) (WHU)
- [ ] [\[2306.04934\] On the Effectiveness of Out-of-Distribution Data in Self-Supervised Long-Tail Learning](https://arxiv.org/abs/2306.04934) (ZJU)
- [ ] [\[2306.04955\] Degraded Polygons Raise Fundamental Questions of Neural Network Perception](https://arxiv.org/abs/2306.04955) (Harvard)
- [ ] [\[2306.05001\] COURIER: Contrastive User Intention Reconstruction for Large-Scale Visual Recommendation](https://arxiv.org/abs/2306.05001) (NJU)
- [ ] [\[2306.05061\] A Dynamic Feature Interaction Framework for Multi-task Visual Perception](https://arxiv.org/abs/2306.05061) (ZJU)
- [ ] [\[2306.05129\] Focus for Free in Density-Based Counting](https://arxiv.org/abs/2306.05129) (A*STAR,)
- [ ] [\[2306.05144\] Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean](https://arxiv.org/abs/2306.05144) (NIPS)
- [ ] [\[2306.05178\] SyncDiffusion: Coherent Montage via Synchronized Joint Diffusions](https://arxiv.org/abs/2306.05178) (NIPS)
- [ ] [\[2306.05225\] Boosting Adversarial Transferability by Achieving Flat Local Maxima](https://arxiv.org/abs/2306.05225) (Xidian, NIPS)
- [ ] [\[2306.05236\] Population-Based Evolutionary Gaming for Unsupervised Person Re-identification](https://arxiv.org/abs/2306.05236) (Peking)
- [ ] [\[2306.05238\] SparseTrack: Multi-Object Tracking by Performing Scene Decomposition based on Pseudo-Depth](https://arxiv.org/abs/2306.05238) (HUST)
- [ ] [\[2306.05239\] Point-Voxel Absorbing Graph Representation Learning for Event Stream based Recognition](https://arxiv.org/abs/2306.05239) (Peking)
- [ ] [\[2306.05254\] Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation](https://arxiv.org/abs/2306.05254) (NWPU)
- [ ] [\[2306.05357\] Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models](https://arxiv.org/abs/2306.05357) (ICCV)
- [ ] [\[2306.05381\] FollowNet: A Comprehensive Benchmark for Car-Following Behavior Modeling](https://arxiv.org/abs/2306.05381) (HKUST)
- [ ] [\[2306.05407\] SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding](https://arxiv.org/abs/2306.05407) (Google, NIPS)
- [ ] [\[2306.05418\] Weakly Supervised 3D Object Detection with Multi-Stage Generalization](https://arxiv.org/abs/2306.05418) (IA CAS)
- [ ] [\[2306.05421\] Stochastic Multi-Person 3D Motion Forecasting](https://arxiv.org/abs/2306.05421) (ICLR)
- [ ] [\[2306.05422\] Tracking Everything Everywhere All at Once](https://arxiv.org/abs/2306.05422) (ICCV)
- [ ] [\[2306.05423\] ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process](https://arxiv.org/abs/2306.05423) (CUHK, ICLR)
- [ ] [\[2306.05425\] MIMIC-IT: Multi-Modal In-Context Instruction Tuning](https://arxiv.org/abs/2306.05425) (Microsoft)
- [ ] [\[2306.05442\] FlowFormer: A Transformer Architecture and Its Masked Cost Volume Autoencoding for Optical Flow](https://arxiv.org/abs/2306.05442) (Tsinghua)
- [ ] [\[2306.05493\] Multi-Modal Classifiers for Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.05493) (Oxford, ICML)
- [ ] [\[2306.05526\] Learning Fine-grained View-Invariant Representations from Unpaired Ego-Exo Videos via Temporal Alignment](https://arxiv.org/abs/2306.05526) (NIPS)
- [ ] [\[2306.05584\] Multi-body SE(3) Equivariance for Unsupervised Rigid Segmentation and Motion Estimation](https://arxiv.org/abs/2306.05584) (NIPS)
- [ ] [\[2306.05612\] Spatial Re-parameterization for N:M Sparsity](https://arxiv.org/abs/2306.05612) (Xiamen)
- [ ] [\[2306.05671\] Topology-Aware Uncertainty for Image Segmentation](https://arxiv.org/abs/2306.05671) (NIPS)
- [ ] [\[2306.05675\] Illumination Controllable Dehazing Network based on Unsupervised Retinex Embedding](https://arxiv.org/abs/2306.05675) (HKUST)
- [ ] [\[2306.05689\] Single-Stage Visual Relationship Learning using Conditional Queries](https://arxiv.org/abs/2306.05689) (NIPS)
- [ ] [\[2306.05691\] DIFT: Dynamic Iterative Field Transforms for Memory Efficient Optical Flow](https://arxiv.org/abs/2306.05691) (CVPR)
- [ ] [\[2306.05718\] Learning Domain-Aware Detection Head with Prompt Tuning](https://arxiv.org/abs/2306.05718) (IS CAS, NIPS)
- [ ] [\[2306.05720\] Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model](https://arxiv.org/abs/2306.05720) (Harvard)
- [ ] [\[2306.05832\] Sketch Beautification: Learning Part Beautification and Structure Refinement for Sketches of Man-made Objects](https://arxiv.org/abs/2306.05832) (ICT CAS)
- [ ] [\[2306.05888\] TrajectoryFormer: 3D Object Tracking Transformer with Predictive Trajectory Hypotheses](https://arxiv.org/abs/2306.05888) (ICCV)
- [ ] [\[2306.05963\] Adaptive Contextual Perception: How to Generalize to New Backgrounds and Ambiguous Objects](https://arxiv.org/abs/2306.05963) (NIPS)
- [ ] [\[2306.06023\] DetZero: Rethinking Offboard 3D Object Detection with Long-term Sequential Point Clouds](https://arxiv.org/abs/2306.06023) (ICCV)
- [ ] [\[2306.06044\] GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields](https://arxiv.org/abs/2306.06044) (SIGGRAPH)
- [ ] [\[2306.06062\] Neural FIM for learning Fisher Information Metrics from point cloud data](https://arxiv.org/abs/2306.06062) (Yale)
- [ ] [\[2306.06078\] Cheating off your neighbors: Improving activity recognition through corroboration](https://arxiv.org/abs/2306.06078) (UT Austin)
- [ ] [\[2306.06089\] Computational Flash Photography through Intrinsics](https://arxiv.org/abs/2306.06089) (CVPR)
- [ ] [\[2306.06093\] HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork](https://arxiv.org/abs/2306.06093) (MIT)
- [ ] [\[2306.06113\] SAM-helps-Shadow:When Segment Anything Model meet shadow removal](https://arxiv.org/abs/2306.06113) (Tianjin)
- [ ] [\[2306.06157\] Fault Localization for Buggy Deep Learning Framework Conversions in Image Recognition](https://arxiv.org/abs/2306.06157) (University of Edinburgh)
- [ ] [\[2306.06189\] FasterViT: Fast Vision Transformers with Hierarchical Attention](https://arxiv.org/abs/2306.06189) (ICLR)
- [ ] [\[2306.06208\] DeltaNN: Assessing the Impact of Computational Environment Parameters on the Performance of Image Recognition Models](https://arxiv.org/abs/2306.06208) (University of Edinburgh)
- [ ] [\[2306.06209\] Backdoor Attack with Sparse and Invisible Trigger](https://arxiv.org/abs/2306.06209) (WHU)
- [ ] [\[2306.06210\] Single-Model Attribution of Generative Models Through Final-Layer Inversion](https://arxiv.org/abs/2306.06210) (ICML)
- [ ] [\[2306.06289\] SegViTv2: Exploring Efficient and Continual Semantic Segmentation with Plain Vision Transformers](https://arxiv.org/abs/2306.06289) (ZJU)
- [ ] [\[2306.06306\] DocumentCLIP: Linking Figures and Main Body Text in Reflowed Documents](https://arxiv.org/abs/2306.06306) (UMD)
- [ ] [\[2306.06323\] Learning Joint Latent Space EBM Prior Model for Multi-layer Generator](https://arxiv.org/abs/2306.06323) (UCLA)
- [ ] [\[2306.06359\] NeRFool: Uncovering the Vulnerability of Generalizable Neural Radiance Fields against Adversarial Perturbations](https://arxiv.org/abs/2306.06359) (GIT, ICML)
- [ ] [\[2306.06360\] 3D reconstruction using Structure for Motion](https://arxiv.org/abs/2306.06360) (UMD)
- [ ] [\[2306.06365\] FalconNet: Factorization for the Light-weight ConvNets](https://arxiv.org/abs/2306.06365) (NJU)
- [ ] [\[2306.06388\] From NeRFLiX to NeRFLiX++: A General NeRF-Agnostic Restorer Paradigm](https://arxiv.org/abs/2306.06388) (CUHK, TPAMI)
- [ ] [\[2306.06494\] Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark](https://arxiv.org/abs/2306.06494) (PolyU)
- [ ] [\[2306.06577\] Semantically-aware Mask CycleGAN for Translating Artistic Portraits to Photo-realistic Visualizations](https://arxiv.org/abs/2306.06577) (HKUST)
- [ ] [\[2306.06584\] Compositional Prototypical Networks for Few-Shot Classification](https://arxiv.org/abs/2306.06584) (UCAS)
- [ ] [\[2306.06634\] Adaptive Multi-Teacher Knowledge Distillation with Meta-Learning](https://arxiv.org/abs/2306.06634) (ZJU)
- [ ] [\[2306.06635\] 2-D SSM: A General Spatial Layer for Visual Transformers](https://arxiv.org/abs/2306.06635) (Tel Aviv)
- [ ] [\[2306.06638\] Face0: Instantaneously Conditioning a Text-to-Image Model on a Face](https://arxiv.org/abs/2306.06638) (Google)
- [ ] [\[2306.06663\] LF-PGVIO: A Visual-Inertial-Odometry Framework for Large Field-of-View Cameras using Points and Geodesic Segments](https://arxiv.org/abs/2306.06663) (ZJU)
- [ ] [\[2306.06687\] LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark](https://arxiv.org/abs/2306.06687) (Shanghai AI Lab, NIPS)
- [ ] [\[2306.06717\] PWR-Align: Leveraging Part-Whole Relationships for Part-wise Rigid Point Cloud Registration in Mixed Reality Applications](https://arxiv.org/abs/2306.06717) (CVPR)
- [ ] [\[2306.06803\] Stable Remaster: Bridging the Gap Between Old Content and New Displays](https://arxiv.org/abs/2306.06803) (UT Austin)
- [ ] [\[2306.06934\] Scale-Rotation-Equivariant Lie Group Convolution Neural Networks (Lie Group-CNNs)](https://arxiv.org/abs/2306.06934) (HIT)
- [ ] [\[2306.06963\] Feature Fusion from Head to Tail for Long-Tailed Visual Recognition](https://arxiv.org/abs/2306.06963) (BU)
- [ ] [\[2306.06991\] Fast Diffusion Model](https://arxiv.org/abs/2306.06991) (NTU)
- [ ] [\[2306.06997\] Slot-VAE: Object-Centric Scene Generation with Slot Attention](https://arxiv.org/abs/2306.06997) (ICML)
- [ ] [\[2306.07005\] AI-Generated Image Detection using a Cross-Attention Enhanced Dual-Stream Network](https://arxiv.org/abs/2306.07005) (SYSU)
- [ ] [\[2306.07030\] Resource Efficient Neural Networks Using Hessian Based Pruning](https://arxiv.org/abs/2306.07030) (NTU)
- [ ] [\[2306.07087\] MaskedFusion360: Reconstruct LiDAR Data by Querying Camera Features](https://arxiv.org/abs/2306.07087) (ICLR)
- [ ] [\[2306.07096\] Global and Local Semantic Completion Learning for Vision-Language Pre-training](https://arxiv.org/abs/2306.07096) (Tsinghua)
- [ ] [\[2306.07178\] Frequency-Based Vulnerability Analysis of Deep Learning Models against Image Corruptions](https://arxiv.org/abs/2306.07178) (EPFL)
- [ ] [\[2306.07197\] AROID: Improving Adversarial Robustness Through Online Instance-Wise Data Augmentation](https://arxiv.org/abs/2306.07197) (Imperial)
- [ ] [\[2306.07257\] MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images](https://arxiv.org/abs/2306.07257) (UESTC)
- [ ] [\[2306.07272\] Zero-shot Composed Text-Image Retrieval](https://arxiv.org/abs/2306.07272) (SJTU)
- [ ] [\[2306.07276\] Transcendental Idealism of Planner: Evaluating Perception from Planning Perspective for Autonomous Driving](https://arxiv.org/abs/2306.07276) (ICML)
- [ ] [\[2306.07280\] Controlling Text-to-Image Diffusion by Orthogonal Finetuning](https://arxiv.org/abs/2306.07280) (NIPS)
- [ ] [\[2306.07282\] Waffling around for Performance: Visual Classification with Random Words and Broad Concepts](https://arxiv.org/abs/2306.07282) (ICCV)
- [ ] [\[2306.07437\] Instant Multi-View Head Capture through Learnable Registration](https://arxiv.org/abs/2306.07437) (CVPR)
- [ ] [\[2306.07470\] Reviving Shift Equivariance in Vision Transformers](https://arxiv.org/abs/2306.07470) (UMD)
- [ ] [\[2306.07476\] AniFaceDrawing: Anime Portrait Exploration during Your Sketching](https://arxiv.org/abs/2306.07476) (SIGGRAPH)
- [ ] [\[2306.07483\] Semi-supervised learning made simple with self-supervised clustering](https://arxiv.org/abs/2306.07483) (CVPR)
- [ ] [\[2306.07490\] Top-Down Framework for Weakly-supervised Grounded Image Captioning](https://arxiv.org/abs/2306.07490) (NTU)
- [ ] [\[2306.07553\] DenseLight: Efficient Control for Large-scale Traffic Signals with Dense Feedback](https://arxiv.org/abs/2306.07553) (PolyU)
- [ ] [\[2306.07579\] Parametric Implicit Face Representation for Audio-Driven Facial Reenactment](https://arxiv.org/abs/2306.07579) (SYSU, CVPR)
- [ ] [\[2306.07581\] Binary Radiance Fields](https://arxiv.org/abs/2306.07581) (POSTECH, NIPS)
- [ ] [\[2306.07646\] Enhanced Multimodal Representation Learning with Cross-modal KD](https://arxiv.org/abs/2306.07646) (CVPR)
- [ ] [\[2306.07684\] Lookaround Optimizer: $k$ steps around, 1 step average](https://arxiv.org/abs/2306.07684) (NIPS)
- [ ] [\[2306.07703\] E2E-LOAD: End-to-End Long-form Online Action Detection](https://arxiv.org/abs/2306.07703) (ShanghaiTech)
- [ ] [\[2306.07716\] Dynamically Masked Discriminator for Generative Adversarial Networks](https://arxiv.org/abs/2306.07716) (NIPS)
- [ ] [\[2306.07783\] Compositionally Equivariant Representation Learning](https://arxiv.org/abs/2306.07783) (University of Edinburgh)
- [ ] [\[2306.07831\] Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology Images](https://arxiv.org/abs/2306.07831) (MIT, CVPR)
- [ ] [\[2306.07879\] Rethinking pose estimation in crowds: overcoming the detection information-bottleneck and ambiguity](https://arxiv.org/abs/2306.07879) (EPFL, ICCV)
- [ ] [\[2306.07881\] Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data](https://arxiv.org/abs/2306.07881) (ICCV)
- [ ] [\[2306.07915\] Image Captioners Are Scalable Vision Learners Too](https://arxiv.org/abs/2306.07915) (NIPS)
- [ ] [\[2306.07952\] MOFI: Learning Image Representations from Noisy Entity Annotated Images](https://arxiv.org/abs/2306.07952) (ICLR)
- [ ] [\[2306.07954\] Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation](https://arxiv.org/abs/2306.07954) (SIGGRAPH)
- [ ] [\[2306.07957\] Hidden Biases of End-to-End Driving Models](https://arxiv.org/abs/2306.07957) (ICCV)
- [ ] [\[2306.07969\] GeneCIS: A Benchmark for General Conditional Image Similarity](https://arxiv.org/abs/2306.07969) (CVPR)
- [ ] [\[2306.07970\] Neural Scene Chronology](https://arxiv.org/abs/2306.07970) (CVPR)
- [ ] [\[2306.07998\] Contrastive Attention Networks for Attribution of Early Modern Print](https://arxiv.org/abs/2306.07998) (UCSD)
- [ ] [\[2306.08045\] Efficient 3D Semantic Segmentation with Superpoint Transformer](https://arxiv.org/abs/2306.08045) (ICCV)
- [ ] [\[2306.08068\] DORSal: Diffusion for Object-centric Representations of Scenes et al](https://arxiv.org/abs/2306.08068) (Google, ICLR)
- [ ] [\[2306.08103\] Generating Images with 3D Annotations Using Diffusion Models](https://arxiv.org/abs/2306.08103) (JHU, ICLR)
- [ ] [\[2306.08129\] AVIS: Autonomous Visual Information Seeking with Large Language Model Agent](https://arxiv.org/abs/2306.08129) (NIPS)
- [ ] [\[2306.08247\] Diffusion in Diffusion: Cyclic One-Way Diffusion for Text-Vision-Conditioned Generation](https://arxiv.org/abs/2306.08247) (Princeton, ICLR)
- [ ] [\[2306.08257\] On the Robustness of Latent Diffusion Models](https://arxiv.org/abs/2306.08257) (SYSU)
- [ ] [\[2306.08275\] C$^3$PS: Context-aware Conditional Cross Pseudo Supervision for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2306.08275) (SJTU)
- [ ] [\[2306.08276\] TryOnDiffusion: A Tale of Two UNets](https://arxiv.org/abs/2306.08276) (CVPR)
- [ ] [\[2306.08330\] Multimodal Optimal Transport-based Co-Attention Transformer with Global Structure Consistency for Survival Prediction](https://arxiv.org/abs/2306.08330) (ICCV)
- [ ] [\[2306.08370\] Object Detection in Hyperspectral Image via Unified Spectral-Spatial Feature Aggregation](https://arxiv.org/abs/2306.08370) (NUDT)
- [ ] [\[2306.08487\] Recognizing Unseen Objects via Multimodal Intensive Knowledge Graph Propagation](https://arxiv.org/abs/2306.08487) (USTC)
- [ ] [\[2306.08528\] Predict to Detect: Prediction-guided 3D Object Detection using Sequential Images](https://arxiv.org/abs/2306.08528) (ICCV)
- [ ] [\[2306.08565\] Reliable Evaluation of Adversarial Transferability](https://arxiv.org/abs/2306.08565) (Oxford)
- [ ] [\[2306.08593\] Heterogeneous Continual Learning](https://arxiv.org/abs/2306.08593) (NYU, CVPR)
- [ ] [\[2306.08609\] TomoSAM: a 3D Slicer extension using SAM for tomography segmentation](https://arxiv.org/abs/2306.08609) (NASA)
- [ ] [\[2306.08625\] RRSIS: Referring Remote Sensing Image Segmentation](https://arxiv.org/abs/2306.08625) (TUM)
- [ ] [\[2306.08637\] TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement](https://arxiv.org/abs/2306.08637) (ICCV)
- [ ] [\[2306.08645\] Training-free Diffusion Model Adaptation for Variable-Sized Text-to-Image Synthesis](https://arxiv.org/abs/2306.08645) (NIPS)
- [ ] [\[2306.08659\] Explore In-Context Learning for 3D Point Cloud Understanding](https://arxiv.org/abs/2306.08659) (SYSU)
- [ ] [\[2306.08687\] Norm-guided latent space exploration for text-to-image generation](https://arxiv.org/abs/2306.08687) (NIPS)
- [ ] [\[2306.08713\] What can a cook in Italy teach a mechanic in India? Action Recognition Generalisation Over Scenarios and Locations](https://arxiv.org/abs/2306.08713) (ICCV)
- [ ] [\[2306.08731\] EPIC Fields: Marrying 3D Geometry and Video Understanding](https://arxiv.org/abs/2306.08731) (NIPS)
- [ ] [\[2306.08736\] LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation](https://arxiv.org/abs/2306.08736) (CVPR)
- [ ] [\[2306.08751\] Improving Selective Visual Question Answering by Learning from Your Peers](https://arxiv.org/abs/2306.08751) (CVPR)
- [ ] [\[2306.08789\] Efficient Token-Guided Image-Text Retrieval with Consistent Multimodal Contrastive Training](https://arxiv.org/abs/2306.08789) (Alibaba)
- [ ] [\[2306.08792\] Graph Convolution Based Efficient Re-Ranking for Visual Retrieval](https://arxiv.org/abs/2306.08792) (Alibaba)
- [ ] [\[2306.08814\] A Self-Supervised Miniature One-Shot Texture Segmentation (MOSTS) Model for Real-Time Robot Navigation and Embedded Applications](https://arxiv.org/abs/2306.08814) (Illinois)
- [ ] [\[2306.08832\] Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Compositional Understanding](https://arxiv.org/abs/2306.08832) (CVPR)
- [ ] [\[2306.08865\] One-Shot Learning of Visual Path Navigation for Autonomous Vehicles](https://arxiv.org/abs/2306.08865) (Google)
- [ ] [\[2306.08887\] SplatFlow: Learning Multi-frame Optical Flow via Splatting](https://arxiv.org/abs/2306.08887) (NUDT)
- [ ] [\[2306.08889\] Dissecting Multimodality in VideoQA Transformer Models by Impairing Modality Fusion](https://arxiv.org/abs/2306.08889) (A*STAR,, ICML)
- [ ] [\[2306.08893\] LOVM: Language-Only Vision Model Selection](https://arxiv.org/abs/2306.08893) (Stanford)
- [ ] [\[2306.08958\] Temporally-Extended Prompts Optimization for SAM in Interactive Medical Image Segmentation](https://arxiv.org/abs/2306.08958) (SJTU)
- [ ] [\[2306.08964\] Exploring Multi-Timestep Multi-Stage Diffusion Features for Hyperspectral Image Classification](https://arxiv.org/abs/2306.08964) (Fudan)
- [ ] [\[2306.08990\] Emotional Speech-Driven Animation with Content-Emotion Disentanglement](https://arxiv.org/abs/2306.08990) (MPI, SIGGRAPH)
- [ ] [\[2306.09001\] SSCBench: Monocular 3D Semantic Scene Completion Benchmark in Street Views](https://arxiv.org/abs/2306.09001) (NYU)
- [ ] [\[2306.09008\] Exploring the Application of Large-scale Pre-trained Models on Adverse Weather Removal](https://arxiv.org/abs/2306.09008) (Alibaba)
- [ ] [\[2306.09012\] Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization](https://arxiv.org/abs/2306.09012) (ICCV)
- [ ] [\[2306.09035\] Improving Explainability of Disentangled Representations using Multipath-Attribution Mappings](https://arxiv.org/abs/2306.09035) (ETH)
- [ ] [\[2306.09077\] Estimating Generic 3D Room Structures from 2D Annotations](https://arxiv.org/abs/2306.09077) (NIPS)
- [ ] [\[2306.09085\] COSA: Concatenated Sample Pretrained Vision-Language Foundation Model](https://arxiv.org/abs/2306.09085) (IA CAS)
- [ ] [\[2306.09098\] Contrast, Stylize and Adapt: Unsupervised Contrastive Learning Framework for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2306.09098) (SJTU)
- [ ] [\[2306.09109\] NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations](https://arxiv.org/abs/2306.09109) (NIPS)
- [ ] [\[2306.09172\] Action Sensitivity Learning for the Ego4D Episodic Memory Challenge 2023](https://arxiv.org/abs/2306.09172) (ZJU)
- [ ] [\[2306.09196\] Infrastructure Crack Segmentation: Boundary Guidance Method and Benchmark Dataset](https://arxiv.org/abs/2306.09196) (HKUST)
- [ ] [\[2306.09224\] Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories](https://arxiv.org/abs/2306.09224) (Google, ICCV)
- [ ] [\[2306.09244\] Text Promptable Surgical Instrument Segmentation with Vision-Language Models](https://arxiv.org/abs/2306.09244) (Tongji, NIPS)
- [ ] [\[2306.09265\] LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2306.09265) (Shanghai AI Lab)
- [ ] [\[2306.09266\] A9 Intersection Dataset: All You Need for Urban 3D Camera-LiDAR Roadside Perception](https://arxiv.org/abs/2306.09266) (TUM)
- [ ] [\[2306.09269\] Zero-Shot Anomaly Detection with Pre-trained Segmentation Models](https://arxiv.org/abs/2306.09269) (Imperial)
- [ ] [\[2306.09305\] Fast Training of Diffusion Models with Masked Transformers](https://arxiv.org/abs/2306.09305) (Caltech)
- [ ] [\[2306.09310\] Infinite Photorealistic Worlds using Procedural Generation](https://arxiv.org/abs/2306.09310) (CVPR)
- [ ] [\[2306.09327\] Language-Guided Music Recommendation for Video via Prompt Analogies](https://arxiv.org/abs/2306.09327) (Illinois, CVPR)
- [ ] [\[2306.09345\] Evaluating Data Attribution for Text-to-Image Models](https://arxiv.org/abs/2306.09345) (ICCV)
